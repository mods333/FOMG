{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pickle\n",
    "from loaders import *\n",
    "from episode import *\n",
    "from dataset import *\n",
    "\n",
    "from common import *\n",
    "from model import Model\n",
    "from meta_learner import MetaLearner\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = load_sampler_from_config(\"../src/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update = 10\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Is the tokenizer 1 indexed?\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2 # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "\n",
    "_loader = Loader(502) # 500 + SOS + EOS\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "encoding_size = 500\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "\n",
    "lr = 0.001\n",
    "baseline = Model(vocabulary_size, \n",
    "              encoding_size, \n",
    "              vocabulary_size,\n",
    "              learning_rate=lr)\n",
    "baseline.load_state_dict(torch.load('../models/baseline_e-4_9000'))\n",
    "\n",
    "meta_learner = MetaLearner(Model,(vocabulary_size,encoding_size,vocabulary_size,lr), lr, num_update)\n",
    "meta_learner.load_state_dict(torch.load('../models/maml_e-4_3000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data.sampler as sampler\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "use_cuda = True\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 + 2  # 4708 + 1\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        _, hidden_out = self.lstm(input, hidden) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "# In[3]:\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        #output = input\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output[0], hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 learning_rate,\n",
    "                 embeddings=one_hot_embeddings):\n",
    "        super(Model,self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.encoder = EncoderLSTM(input_size, hidden_size)\n",
    "        self.decoder = DecoderLSTM(input_size, hidden_size, output_size)\n",
    "        #self.encoder_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "        #self.decoder_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "        if use_cuda:\n",
    "            self.encoder = self.encoder.cuda()\n",
    "            self.decoder = self.decoder.cuda()\n",
    "            self.criterion = self.criterion.cuda()\n",
    "\n",
    "    def forward(self, token_seqs):\n",
    "        loss = 0\n",
    "        start = 50\n",
    "        batch_size = len(token_seqs)\n",
    "        #seq_len = len(token_seqs[0])\n",
    "        seq_len = 150\n",
    "        #self.encoder_optimizer.zero_grad()\n",
    "        #self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_hidden = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        encoder_output = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            encoder_output = encoder_output.cuda()\n",
    "\n",
    "        hidden = (encoder_hidden, encoder_output)\n",
    "        for i in np.arange(start-1, -1, -1):\n",
    "            token_batch = np.array(self.embeddings[token_seqs[:, i]])\n",
    "            encoder_input = Variable(torch.from_numpy(token_batch)).view(1, batch_size, -1).double()\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            #print(\"encoder_input: %d\" % (np.where(encoder_input.data==1)[2][0]))\n",
    "            hidden = self.encoder(encoder_input, hidden)\n",
    "        encoder_hidden, _ = hidden\n",
    "\n",
    "        token_batch = np.array(self.embeddings[token_seqs[:, start-1]])\n",
    "        decoder_input = Variable(torch.from_numpy(token_batch)).double()\n",
    "        decoder_output = Variable(self.decoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            decoder_output = decoder_output.cuda()\n",
    "\n",
    "        hidden = (encoder_hidden, decoder_output)\n",
    "        for i in range(start, seq_len+1):\n",
    "            decoder_input = decoder_input.squeeze().view(1, batch_size, -1)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            #print(\"decoder_input: %d\" % (np.where(decoder_input.data==1)[2][0]))\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            #print(\"prediction: %d\" % (int(decoder_output.topk(1)[1])))\n",
    "            if i < seq_len:\n",
    "                seq_var = token_seqs[:, i]\n",
    "            else:\n",
    "                seq_var = [EOS_TOKEN]*batch_size\n",
    "\n",
    "            target = Variable(torch.from_numpy(np.array(seq_var))).long()\n",
    "            target = target.cuda() if use_cuda else target\n",
    "            loss += self.criterion(decoder_output, target)\n",
    "\n",
    "            # Teacher forcing\n",
    "            decoder_input = Variable(torch.from_numpy(np.array(self.embeddings[seq_var]))).double()\n",
    "\n",
    "        loss = torch.sum(loss)/batch_size\n",
    "        #loss.backward()\n",
    "        #self.encoder_optimizer.step()\n",
    "        #self.decoder_optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def map_inference(self, token_seqs):\n",
    "        batch_size = len(token_seqs)\n",
    "        seq_len = len(token_seqs[0])\n",
    "        encoder_hidden = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        encoder_output = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            encoder_output = encoder_output.cuda()\n",
    "\n",
    "        hidden = (encoder_output, encoder_hidden)\n",
    "        for i in np.arange(seq_len-1, 0, -1):\n",
    "            token_batch = np.array(self.embeddings[token_seqs[:, i]])\n",
    "            encoder_input = Variable(torch.from_numpy(token_batch)).view(1, batch_size, -1).double()\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            hidden = self.encoder(encoder_input, hidden)\n",
    "\n",
    "        encoder_output, encoder_hidden = hidden\n",
    "\n",
    "        token_batch = np.array(self.embeddings[[SOS_TOKEN]*batch_size])\n",
    "        decoder_output = Variable(self.decoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            decoder_output = decoder_output.cuda()\n",
    "\n",
    "        hidden = (decoder_output, encoder_hidden)\n",
    "\n",
    "        pred_seqs = None\n",
    "        for i in range(150):\n",
    "            decoder_input = Variable(torch.from_numpy(token_batch)).double()\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            decoder_input = decoder_input.squeeze().view(1, batch_size, -1)\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            #print(\"Iteration: %d, Prediction: %d\" % (i, token))\n",
    "            if pred_seqs is None:\n",
    "                pred_seqs = topi.cpu().numpy()\n",
    "            else:\n",
    "                pred_seqs = np.concatenate((pred_seqs, topi.cpu().numpy()), axis=1)\n",
    "            token_batch = np.array(self.embeddings[topi])\n",
    "\n",
    "        return pred_seqs.tolist()\n",
    "        \n",
    "    def sample_inference(self, token_seqs):\n",
    "        softmax = nn.Softmax()\n",
    "        batch_size = len(token_seqs)\n",
    "        seq_len = len(token_seqs[0])\n",
    "        encoder_hidden = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        encoder_output = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            encoder_output = encoder_output.cuda()\n",
    "        \n",
    "        hidden = (encoder_output, encoder_hidden)\n",
    "        for i in np.arange(seq_len-1, -1, -1):\n",
    "            token_batch = np.array(self.embeddings[token_seqs[:, i]])\n",
    "            encoder_input = Variable(torch.from_numpy(token_batch)).view(1, batch_size, -1).double()\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            hidden = self.encoder(encoder_input, hidden)\n",
    "        \n",
    "        encoder_output, encoder_hidden = hidden\n",
    "            \n",
    "        token_batch = np.array(self.embeddings[[SOS_TOKEN]*batch_size])\n",
    "        decoder_output = Variable(self.decoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            decoder_output = decoder_output.cuda()\n",
    "        \n",
    "        hidden = (decoder_output, encoder_hidden)\n",
    "\n",
    "        pred_seqs = None\n",
    "        for i in range(250):\n",
    "            decoder_input = Variable(torch.from_numpy(token_batch)).double()\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            decoder_input = decoder_input.squeeze().view(1, batch_size, -1)\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            \n",
    "            output = softmax(decoder_output).data.cpu()\n",
    "            output = output.numpy()\n",
    "\n",
    "            cdf = np.cumsum(output)\n",
    "            uniform_sample = np.random.uniform()\n",
    "            \n",
    "            for _index, item in enumerate(cdf):\n",
    "                if uniform_sample > item and uniform_sample <= cdf[_index+1]:\n",
    "                    ni = np.array([[_index]])\n",
    "                    break\n",
    "\n",
    "            if ni != EOS_TOKEN and ni != SOS_TOKEN:    \n",
    "                if pred_seqs is None:\n",
    "                    pred_seqs = ni\n",
    "                else:\n",
    "                    pred_seqs = np.concatenate((pred_seqs, ni), axis=1)\n",
    "                    \n",
    "            token_batch = np.array(self.embeddings[ni])\n",
    "            \n",
    "        return pred_seqs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking Benjamin forever.mid\n",
      "Breaking Benjamin polyamorous.mid\n",
      "Breaking Benjamin diaryofjane(novocaltrack).mid\n",
      "Breaking Benjamin skin.mid\n",
      "Breaking Benjamin breakdown.mid\n",
      "Breaking Benjamin sugarcoat.mid\n",
      "Breaking Benjamin diaryofjane.mid\n",
      "Breaking Benjamin socold.mid\n",
      "Breaking Benjamin enjoythesilence.mid\n",
      "Breaking Benjamin firefly.mid\n",
      "Breaking Benjamin water.mid\n",
      "Breaking Benjamin soonerorlater.mid\n",
      "Breaking Benjamin shallowbay.mid\n",
      "Breaking Benjamin diaryofjane(bestversion).mid\n",
      "Breaking Benjamin blowmeaway.mid\n",
      "Children of Bodom childrenofdecadence.mid\n",
      "Children of Bodom towardsdeadend.mid\n",
      "Children of Bodom sixpounder.mid\n",
      "Children of Bodom oopsididitagain.mid\n",
      "Children of Bodom vivaldissummertheme.mid\n",
      "Children of Bodom hatecrewdeathroll.mid\n",
      "Children of Bodom kissingtheshadows.mid\n",
      "Children of Bodom trashedlost&strungout.mid\n",
      "Children of Bodom dontstopatthetop.mid\n",
      "Children of Bodom aceshigh.mid\n",
      "Children of Bodom silentnightbodomnight.mid\n",
      "Children of Bodom lilbloodredridinhood.mid\n",
      "Children of Bodom hellion.mid\n",
      "Children of Bodom thenail.mid\n",
      "Children of Bodom touchlikeanangelofdeath.mid\n",
      "Boney M riversofbabylon.mid\n",
      "Boney M mabaker2.mid\n",
      "Boney M kalimbadeluna.mid\n",
      "Boney M plantationboy.mid\n",
      "Boney M dreadlockholiday.mid\n",
      "Boney M rasputin.mid\n",
      "Boney M popmuzik.mid\n",
      "Boney M malaika.mid\n",
      "Boney M mabaker.mid\n",
      "Boney M ellute.mid\n",
      "Boney M happysong.mid\n",
      "Boney M daddycool.mid\n",
      "Boney M sunny.mid\n",
      "Boney M braungirl.mid\n",
      "Boney M hoorayhoorayitsaholiday.mid\n",
      "3.342048008044598\n",
      "3.283938447373338\n",
      "3.2399104499570806\n",
      "3.2089480483143906\n",
      "3.185150218790428\n",
      "3.162249977306106\n",
      "3.141220910696067\n",
      "3.121947456672421\n",
      "3.1025215743699253\n",
      "3.0829575414929673\n",
      "3.064635562128256\n",
      "3.046597116916511\n",
      "3.0284202477032767\n",
      "3.0113916518312758\n",
      "2.9938019976411265\n",
      "2.9771888352368183\n",
      "2.9619404983140116\n",
      "2.9462744777750305\n",
      "2.93086427386002\n",
      "2.916061379959205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fomg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:218: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate: baseline_1_1\n",
      "Failed to generate: baseline_1_2\n",
      "Failed to generate: baseline_1_5\n",
      "3.911929411190791\n",
      "3.8958073053085034\n",
      "3.869876849391084\n",
      "3.8343959201274878\n",
      "3.792759107751883\n",
      "3.7543455648137742\n",
      "3.72082748140326\n",
      "3.691284766143192\n",
      "3.667964084404258\n",
      "3.654099387670097\n",
      "3.643576958304864\n",
      "3.628196869091671\n",
      "3.608244370869\n",
      "3.5871095994032554\n",
      "3.5672098850658447\n",
      "3.5492797315486078\n",
      "3.5331243076803003\n",
      "3.518304539803548\n",
      "3.5044126156554323\n",
      "3.491143229992837\n",
      "Failed to generate: maml_1_2\n",
      "Failed to generate: maml_1_5\n",
      "3.2673736091301397\n",
      "3.219899790264635\n",
      "3.1844088633379255\n",
      "3.1612521129570346\n",
      "3.1362439558560555\n",
      "3.1093584675772274\n",
      "3.085748503262981\n",
      "3.062515683433439\n",
      "3.038464545181926\n",
      "3.0154782970586367\n",
      "2.994633955692624\n",
      "2.974619361357701\n",
      "2.9538512697817034\n",
      "2.9324073876923604\n",
      "2.9114876697693943\n",
      "2.890005912177278\n",
      "2.869344562547902\n",
      "2.848069810674803\n",
      "2.8286295630502787\n",
      "2.8114195539638915\n",
      "Failed to generate: baseline_2_1\n",
      "Failed to generate: baseline_2_3\n",
      "Failed to generate: baseline_2_4\n",
      "Failed to generate: baseline_2_5\n",
      "4.025289279754679\n",
      "4.011665009942811\n",
      "3.9881346694816093\n",
      "3.9565381483473563\n",
      "3.918127104104417\n",
      "3.8740635600010016\n",
      "3.825913472839008\n",
      "3.77621284510101\n",
      "3.7292626010880783\n",
      "3.6918226185530028\n",
      "3.6693738566497585\n",
      "3.657281379800532\n",
      "3.6436593501737398\n",
      "3.622428382965479\n",
      "3.595077033638056\n",
      "3.565661372482252\n",
      "3.5375408840278237\n",
      "3.5121619482026767\n",
      "3.489420460241736\n",
      "3.4686509898647024\n",
      "Failed to generate: maml_2_3\n",
      "Failed to generate: maml_2_5\n",
      "3.8418302474104706\n",
      "3.8132278787156846\n",
      "3.788191412084171\n",
      "3.773461108464524\n",
      "3.75263549728396\n",
      "3.730706060431457\n",
      "3.7063397038062833\n",
      "3.6829364483615707\n",
      "3.6602481001159437\n",
      "3.6363377931565606\n",
      "3.6137858601579533\n",
      "3.591629796832133\n",
      "3.5679901820592828\n",
      "3.5438070796105405\n",
      "3.521815485551582\n",
      "3.4981692508981896\n",
      "3.4755753743857727\n",
      "3.454288380196654\n",
      "3.4329914874097955\n",
      "3.4114762242534047\n",
      "Failed to generate: baseline_3_1\n",
      "Failed to generate: baseline_3_2\n",
      "Failed to generate: baseline_3_3\n",
      "Failed to generate: baseline_3_4\n",
      "Failed to generate: baseline_3_5\n",
      "5.123310227726316\n",
      "5.071382286565865\n",
      "4.958139679113775\n",
      "4.718497825923905\n",
      "4.382909826401593\n",
      "4.202460779993938\n",
      "4.124833263871556\n",
      "4.096408979866734\n",
      "4.086206697357798\n",
      "4.083968811802888\n",
      "4.082865265325095\n",
      "4.0796072050968295\n",
      "4.0727893615085655\n",
      "4.062307963042618\n",
      "4.048614230711074\n",
      "4.032410658278231\n",
      "4.014488211512219\n",
      "3.995625617188271\n",
      "3.9765199399052276\n",
      "3.9577266715401636\n",
      "Failed to generate: maml_3_1\n",
      "Failed to generate: maml_3_2\n",
      "Failed to generate: maml_3_3\n",
      "Failed to generate: maml_3_4\n",
      "Failed to generate: maml_3_5\n"
     ]
    }
   ],
   "source": [
    "test_model = Model(vocabulary_size, encoding_size, vocabulary_size, learning_rate=lr)\n",
    "optimizer = torch.optim.Adam(test_model.parameters(), 0.0001)\n",
    "\n",
    "N = 1 # 30 Artists\n",
    "num_updates = 20\n",
    "feed_length = 75\n",
    "for index in range(N):\n",
    "    episode = eps.get_episode()\n",
    "    support = episode.support\n",
    "    query = episode.query\n",
    "\n",
    "    for artist_index in range(support.shape[0]):\n",
    "        \"\"\" Baseline \"\"\"\n",
    "        test_model.load_state_dict(baseline.state_dict())\n",
    "        test_model.train()\n",
    "        for _ in range(num_updates):\n",
    "            # train for 10 iterations\n",
    "            optimizer.zero_grad()\n",
    "            loss = test_model(support[artist_index])\n",
    "            print(loss.data[0]/150)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        test_model.eval()\n",
    "        for inf_index in range(1, query[artist_index].shape[0]+1):\n",
    "            try:\n",
    "                # Make inference for each song\n",
    "                song = query[artist_index][inf_index]\n",
    "                midi = loader.detokenize(np.array(song))\n",
    "                midi.write('baseline_{}_{}_orig.mid'.format(index*3+artist_index+1, inf_index))\n",
    "\n",
    "                midi = loader.detokenize(np.array(song[:feed_length]))\n",
    "                midi.write('baseline_{}_{}_inpu.mid'.format(index*3+artist_index+1, inf_index))\n",
    "\n",
    "                gen_seq = test_model.sample_inference(np.array([song[:feed_length]]))\n",
    "                midi = loader.detokenize(np.append(song[:feed_length], np.array(gen_seq[0])))\n",
    "                midi.write('baseline_{}_{}_pred.mid'.format(index*3+artist_index+1, inf_index))\n",
    "            except:\n",
    "                print('Failed to generate: baseline_{}_{}'.format(index*3+artist_index+1, inf_index))\n",
    "        \"\"\" MAML \"\"\"\n",
    "        test_model.load_state_dict(meta_learner.learner.meta_net.state_dict())\n",
    "        test_model.train()\n",
    "        for _ in range(num_updates):\n",
    "            # train for 10 iterations\n",
    "            optimizer.zero_grad()\n",
    "            loss = test_model(support[artist_index])\n",
    "            print(loss.data[0]/150)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        test_model.eval()\n",
    "        for inf_index in range(1, query[artist_index].shape[0]+1):\n",
    "            try:\n",
    "                # Make inference for each song\n",
    "                song = query[artist_index][inf_index]\n",
    "                midi = loader.detokenize(np.array(song))\n",
    "                midi.write('maml_{}_{}_orig.mid'.format(index*3+artist_index+1, inf_index))\n",
    "\n",
    "                midi = loader.detokenize(np.array(song[:feed_length]))\n",
    "                midi.write('maml_{}_{}_inpu.mid'.format(index*3+artist_index+1, inf_index))\n",
    "\n",
    "                gen_seq = test_model.sample_inference(np.array([song[:feed_length]]))\n",
    "                midi = loader.detokenize(np.append(song[:feed_length], np.array(gen_seq[0])))\n",
    "                midi.write('maml_{}_{}_pred.mid'.format(index*3+artist_index+1, inf_index))\n",
    "            except:\n",
    "                print('Failed to generate: maml_{}_{}'.format(index*3+artist_index+1, inf_index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
