{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from loaders import *\n",
    "from episode import *\n",
    "from dataset import *\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Some global variables \"\"\"\n",
    "_loader = Loader(502) # 500 + SOS + EOS\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Is the tokenizer 1 indexed?\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2 # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "\n",
    "encoding_size = 500\n",
    "one_hot_embeddings = np.eye(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.double()\n",
    "    loss = losses.sum() / length.double().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        _, hidden_out = self.lstm(input, hidden) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output[0], hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 learning_rate,\n",
    "                 embeddings=one_hot_embeddings):\n",
    "        super(Learner,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = EncoderLSTM(input_size, hidden_size)\n",
    "        self.decoder = DecoderLSTM(input_size, hidden_size, output_size)\n",
    "        self.encoder_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "        self.decoder_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, token_seq):\n",
    "        loss = 0\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_hidden = Variable(self.encoder.initHidden()).double()\n",
    "        encoder_output = Variable(self.encoder.initHidden()).double()\n",
    "        if use_cuda:\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            encoder_output = encoder_output.cuda()\n",
    "        \n",
    "        hidden = (encoder_output, encoder_hidden)\n",
    "        token_seq.reverse()\n",
    "        for token in token_seq:\n",
    "            encoder_input = Variable(torch.from_numpy(np.array(self.embeddings[token]))).view(1, 1, -1).double()\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            hidden = self.encoder(encoder_input, hidden)\n",
    "        \n",
    "        encoder_output, encoder_hidden = hidden\n",
    "            \n",
    "        decoder_input = Variable(torch.from_numpy(np.array(self.embeddings[SOS_TOKEN]))).double()\n",
    "        decoder_output = Variable(self.decoder.initHidden()).double()\n",
    "        if use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_output = decoder_output.cuda()\n",
    "        \n",
    "        hidden = (decoder_output, encoder_hidden)\n",
    "        token_seq.reverse()\n",
    "        targets = token_seq + [EOS_TOKEN]\n",
    "        seq_var = Variable(torch.from_numpy(np.array(targets)))\n",
    "        seq_var = seq_var.cuda() if use_cuda else seq_var\n",
    "        for i, token in enumerate(targets):\n",
    "            decoder_input = decoder_input.squeeze().view(1, 1, -1)\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            loss += self.criterion(decoder_output, seq_var[i])\n",
    "            if token == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "            # Teacher forcing\n",
    "            target = Variable(torch.from_numpy(np.array(self.embeddings[token]))).view(1, -1).double()\n",
    "            if use_cuda:\n",
    "                target = target.cuda()\n",
    "            decoder_input = target\n",
    "        \n",
    "        loss.backward()\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "        return loss.data[0] / len(token_seq)\n",
    "    \n",
    "    def map_inference(self, token_seq):\n",
    "        encoder_hidden = Variable(self.encoder.initHidden()).double()\n",
    "        encoder_output = Variable(self.encoder.initHidden()).double()\n",
    "        if use_cuda:\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            encoder_output = encoder_output.cuda()\n",
    "        \n",
    "        hidden = (encoder_output, encoder_hidden)\n",
    "        token_seq.reverse()\n",
    "        for token in token_seq:\n",
    "            encoder_input = Variable(torch.from_numpy(np.array(self.embeddings[token]))).view(1, 1, -1).double()\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            hidden = self.encoder(encoder_input, hidden)\n",
    "        \n",
    "        encoder_output, encoder_hidden = hidden\n",
    "            \n",
    "        token = SOS_TOKEN\n",
    "        \n",
    "        decoder_output = Variable(self.decoder.initHidden()).double()\n",
    "        if use_cuda:\n",
    "            decoder_output = decoder_output.cuda()\n",
    "        \n",
    "        hidden = (decoder_output, encoder_hidden)\n",
    "\n",
    "        token_seq = []\n",
    "        for i in range(500):\n",
    "            decoder_input = Variable(torch.from_numpy(np.array(self.embeddings[token]))).double()\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            decoder_input = decoder_input.squeeze().view(1, 1, -1)\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            token = int(topi)\n",
    "            print(\"Iteration: %d, Prediction: %d\" % (i, token))\n",
    "            if token == EOS_TOKEN:\n",
    "                break\n",
    "                \n",
    "            token_seq.append(token)\n",
    "            \n",
    "        return token_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenny Kravitz whatgoesaroundcomesaround.mid\n",
      "Lenny Kravitz dontgoandputabulletinyourhead.mid\n",
      "Lenny Kravitz callingallangels.mid\n",
      "Lenny Kravitz alwaysontherun.mid\n",
      "Lenny Kravitz letloverule.mid\n",
      "Lenny Kravitz fieldsofjoy.mid\n",
      "Lenny Kravitz digin.mid\n",
      "Lenny Kravitz again.mid\n",
      "Lenny Kravitz destiny.mid\n",
      "Lenny Kravitz itaintovertillitsover.mid\n",
      "Lenny Kravitz stillnessofheart.mid\n"
     ]
    }
   ],
   "source": [
    "learner = Learner(vocabulary_size, \n",
    "              encoding_size, \n",
    "              vocabulary_size,\n",
    "              learning_rate=0.01)\n",
    "\n",
    "learner = learner.cuda() if use_cuda else learner\n",
    "eps = load_sampler_from_config(\"../src/config.yaml\")\n",
    "song = eps.get_episode().support[0][0]\n",
    "song = list(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n",
      "Iteration: 50\n",
      "Iteration: 51\n",
      "Iteration: 52\n",
      "Iteration: 53\n",
      "Iteration: 54\n",
      "Iteration: 55\n",
      "Iteration: 56\n",
      "Iteration: 57\n",
      "Iteration: 58\n",
      "Iteration: 59\n",
      "Iteration: 60\n",
      "Iteration: 61\n",
      "Iteration: 62\n",
      "Iteration: 63\n",
      "Iteration: 64\n",
      "Iteration: 65\n",
      "Iteration: 66\n",
      "Iteration: 67\n",
      "Iteration: 68\n",
      "Iteration: 69\n",
      "Iteration: 70\n",
      "Iteration: 71\n",
      "Iteration: 72\n",
      "Iteration: 73\n",
      "Iteration: 74\n",
      "Iteration: 75\n",
      "Iteration: 76\n",
      "Iteration: 77\n",
      "Iteration: 78\n",
      "Iteration: 79\n",
      "Iteration: 80\n",
      "Iteration: 81\n",
      "Iteration: 82\n",
      "Iteration: 83\n",
      "Iteration: 84\n",
      "Iteration: 85\n",
      "Iteration: 86\n",
      "Iteration: 87\n",
      "Iteration: 88\n",
      "Iteration: 89\n",
      "Iteration: 90\n",
      "Iteration: 91\n",
      "Iteration: 92\n",
      "Iteration: 93\n",
      "Iteration: 94\n",
      "Iteration: 95\n",
      "Iteration: 96\n",
      "Iteration: 97\n",
      "Iteration: 98\n",
      "Iteration: 99\n",
      "Iteration: 100\n",
      "Iteration: 101\n",
      "Iteration: 102\n",
      "Iteration: 103\n",
      "Iteration: 104\n",
      "Iteration: 105\n",
      "Iteration: 106\n",
      "Iteration: 107\n",
      "Iteration: 108\n",
      "Iteration: 109\n",
      "Iteration: 110\n",
      "Iteration: 111\n",
      "Iteration: 112\n",
      "Iteration: 113\n",
      "Iteration: 114\n",
      "Iteration: 115\n",
      "Iteration: 116\n",
      "Iteration: 117\n",
      "Iteration: 118\n",
      "Iteration: 119\n",
      "Iteration: 120\n",
      "Iteration: 121\n",
      "Iteration: 122\n",
      "Iteration: 123\n",
      "Iteration: 124\n",
      "Iteration: 125\n",
      "Iteration: 126\n",
      "Iteration: 127\n",
      "Iteration: 128\n",
      "Iteration: 129\n",
      "Iteration: 130\n",
      "Iteration: 131\n",
      "Iteration: 132\n",
      "Iteration: 133\n",
      "Iteration: 134\n",
      "Iteration: 135\n",
      "Iteration: 136\n",
      "Iteration: 137\n",
      "Iteration: 138\n",
      "Iteration: 139\n",
      "Iteration: 140\n",
      "Iteration: 141\n",
      "Iteration: 142\n",
      "Iteration: 143\n",
      "Iteration: 144\n",
      "Iteration: 145\n",
      "Iteration: 146\n",
      "Iteration: 147\n",
      "Iteration: 148\n",
      "Iteration: 149\n",
      "Iteration: 150\n",
      "Iteration: 151\n",
      "Iteration: 152\n",
      "Iteration: 153\n",
      "Iteration: 154\n",
      "Iteration: 155\n",
      "Iteration: 156\n",
      "Iteration: 157\n",
      "Iteration: 158\n",
      "Iteration: 159\n",
      "Iteration: 160\n",
      "Iteration: 161\n",
      "Iteration: 162\n",
      "Iteration: 163\n",
      "Iteration: 164\n",
      "Iteration: 165\n",
      "Iteration: 166\n",
      "Iteration: 167\n",
      "Iteration: 168\n",
      "Iteration: 169\n",
      "Iteration: 170\n",
      "Iteration: 171\n",
      "Iteration: 172\n",
      "Iteration: 173\n",
      "Iteration: 174\n",
      "Iteration: 175\n",
      "Iteration: 176\n",
      "Iteration: 177\n",
      "Iteration: 178\n",
      "Iteration: 179\n",
      "Iteration: 180\n",
      "Iteration: 181\n",
      "Iteration: 182\n",
      "Iteration: 183\n",
      "Iteration: 184\n",
      "Iteration: 185\n",
      "Iteration: 186\n",
      "Iteration: 187\n",
      "Iteration: 188\n",
      "Iteration: 189\n",
      "Iteration: 190\n",
      "Iteration: 191\n",
      "Iteration: 192\n",
      "Iteration: 193\n",
      "Iteration: 194\n",
      "Iteration: 195\n",
      "Iteration: 196\n",
      "Iteration: 197\n",
      "Iteration: 198\n",
      "Iteration: 199\n",
      "Iteration: 200\n",
      "Iteration: 201\n",
      "Iteration: 202\n",
      "Iteration: 203\n",
      "Iteration: 204\n",
      "Iteration: 205\n",
      "Iteration: 206\n",
      "Iteration: 207\n",
      "Iteration: 208\n",
      "Iteration: 209\n",
      "Iteration: 210\n",
      "Iteration: 211\n",
      "Iteration: 212\n",
      "Iteration: 213\n",
      "Iteration: 214\n",
      "Iteration: 215\n",
      "Iteration: 216\n",
      "Iteration: 217\n",
      "Iteration: 218\n",
      "Iteration: 219\n",
      "Iteration: 220\n",
      "Iteration: 221\n",
      "Iteration: 222\n",
      "Iteration: 223\n",
      "Iteration: 224\n",
      "Iteration: 225\n",
      "Iteration: 226\n",
      "Iteration: 227\n",
      "Iteration: 228\n",
      "Iteration: 229\n",
      "Iteration: 230\n",
      "Iteration: 231\n",
      "Iteration: 232\n",
      "Iteration: 233\n",
      "Iteration: 234\n",
      "Iteration: 235\n",
      "Iteration: 236\n",
      "Iteration: 237\n",
      "Iteration: 238\n",
      "Iteration: 239\n",
      "Iteration: 240\n",
      "Iteration: 241\n",
      "Iteration: 242\n",
      "Iteration: 243\n",
      "Iteration: 244\n",
      "Iteration: 245\n",
      "Iteration: 246\n",
      "Iteration: 247\n",
      "Iteration: 248\n",
      "Iteration: 249\n",
      "Iteration: 250\n",
      "Iteration: 251\n",
      "Iteration: 252\n",
      "Iteration: 253\n",
      "Iteration: 254\n",
      "Iteration: 255\n",
      "Iteration: 256\n",
      "Iteration: 257\n",
      "Iteration: 258\n",
      "Iteration: 259\n",
      "Iteration: 260\n",
      "Iteration: 261\n",
      "Iteration: 262\n",
      "Iteration: 263\n",
      "Iteration: 264\n",
      "Iteration: 265\n",
      "Iteration: 266\n",
      "Iteration: 267\n",
      "Iteration: 268\n",
      "Iteration: 269\n",
      "Iteration: 270\n",
      "Iteration: 271\n",
      "Iteration: 272\n",
      "Iteration: 273\n",
      "Iteration: 274\n",
      "Iteration: 275\n",
      "Iteration: 276\n",
      "Iteration: 277\n",
      "Iteration: 278\n",
      "Iteration: 279\n",
      "Iteration: 280\n",
      "Iteration: 281\n",
      "Iteration: 282\n",
      "Iteration: 283\n",
      "Iteration: 284\n",
      "Iteration: 285\n",
      "Iteration: 286\n",
      "Iteration: 287\n",
      "Iteration: 288\n",
      "Iteration: 289\n",
      "Iteration: 290\n",
      "Iteration: 291\n",
      "Iteration: 292\n",
      "Iteration: 293\n",
      "Iteration: 294\n",
      "Iteration: 295\n",
      "Iteration: 296\n",
      "Iteration: 297\n",
      "Iteration: 298\n",
      "Iteration: 299\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    print(\"Iteration: %d\" % i)\n",
    "    learner(song)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.state_dict(), \"../models/song.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Prediction: 4248\n",
      "Iteration: 1, Prediction: 564\n",
      "Iteration: 2, Prediction: 4344\n",
      "Iteration: 3, Prediction: 958\n",
      "Iteration: 4, Prediction: 668\n",
      "Iteration: 5, Prediction: 4628\n",
      "Iteration: 6, Prediction: 2612\n",
      "Iteration: 7, Prediction: 567\n",
      "Iteration: 8, Prediction: 4628\n",
      "Iteration: 9, Prediction: 2622\n",
      "Iteration: 10, Prediction: 567\n",
      "Iteration: 11, Prediction: 4628\n",
      "Iteration: 12, Prediction: 2615\n",
      "Iteration: 13, Prediction: 564\n",
      "Iteration: 14, Prediction: 4628\n",
      "Iteration: 15, Prediction: 2612\n",
      "Iteration: 16, Prediction: 567\n",
      "Iteration: 17, Prediction: 4628\n",
      "Iteration: 18, Prediction: 2622\n",
      "Iteration: 19, Prediction: 567\n",
      "Iteration: 20, Prediction: 4628\n",
      "Iteration: 21, Prediction: 2615\n",
      "Iteration: 22, Prediction: 564\n",
      "Iteration: 23, Prediction: 4628\n",
      "Iteration: 24, Prediction: 2612\n",
      "Iteration: 25, Prediction: 567\n",
      "Iteration: 26, Prediction: 4628\n",
      "Iteration: 27, Prediction: 2622\n",
      "Iteration: 28, Prediction: 567\n",
      "Iteration: 29, Prediction: 4628\n",
      "Iteration: 30, Prediction: 2615\n",
      "Iteration: 31, Prediction: 564\n",
      "Iteration: 32, Prediction: 4628\n",
      "Iteration: 33, Prediction: 2612\n",
      "Iteration: 34, Prediction: 567\n",
      "Iteration: 35, Prediction: 4628\n",
      "Iteration: 36, Prediction: 2622\n",
      "Iteration: 37, Prediction: 567\n",
      "Iteration: 38, Prediction: 4628\n",
      "Iteration: 39, Prediction: 2615\n",
      "Iteration: 40, Prediction: 564\n",
      "Iteration: 41, Prediction: 4628\n",
      "Iteration: 42, Prediction: 2612\n",
      "Iteration: 43, Prediction: 567\n",
      "Iteration: 44, Prediction: 4628\n",
      "Iteration: 45, Prediction: 2622\n",
      "Iteration: 46, Prediction: 567\n",
      "Iteration: 47, Prediction: 4628\n",
      "Iteration: 48, Prediction: 2615\n",
      "Iteration: 49, Prediction: 564\n",
      "Iteration: 50, Prediction: 4628\n",
      "Iteration: 51, Prediction: 2612\n",
      "Iteration: 52, Prediction: 567\n",
      "Iteration: 53, Prediction: 4628\n",
      "Iteration: 54, Prediction: 2622\n",
      "Iteration: 55, Prediction: 4188\n",
      "Iteration: 56, Prediction: 322\n",
      "Iteration: 57, Prediction: 564\n",
      "Iteration: 58, Prediction: 4628\n",
      "Iteration: 59, Prediction: 2370\n",
      "Iteration: 60, Prediction: 2612\n",
      "Iteration: 61, Prediction: 4180\n",
      "Iteration: 62, Prediction: 322\n",
      "Iteration: 63, Prediction: 574\n",
      "Iteration: 64, Prediction: 4628\n",
      "Iteration: 65, Prediction: 2370\n",
      "Iteration: 66, Prediction: 2622\n",
      "Iteration: 67, Prediction: 4172\n",
      "Iteration: 68, Prediction: 322\n",
      "Iteration: 69, Prediction: 574\n",
      "Iteration: 70, Prediction: 4628\n",
      "Iteration: 71, Prediction: 2368\n",
      "Iteration: 72, Prediction: 2622\n",
      "Iteration: 73, Prediction: 2716\n",
      "Iteration: 74, Prediction: 4172\n",
      "Iteration: 75, Prediction: 327\n",
      "Iteration: 76, Prediction: 567\n",
      "Iteration: 77, Prediction: 4628\n",
      "Iteration: 78, Prediction: 2370\n",
      "Iteration: 79, Prediction: 2622\n",
      "Iteration: 80, Prediction: 4172\n",
      "Iteration: 81, Prediction: 322\n",
      "Iteration: 82, Prediction: 574\n",
      "Iteration: 83, Prediction: 4628\n",
      "Iteration: 84, Prediction: 2368\n",
      "Iteration: 85, Prediction: 2622\n",
      "Iteration: 86, Prediction: 2716\n",
      "Iteration: 87, Prediction: 4172\n",
      "Iteration: 88, Prediction: 327\n",
      "Iteration: 89, Prediction: 567\n",
      "Iteration: 90, Prediction: 4628\n",
      "Iteration: 91, Prediction: 2370\n",
      "Iteration: 92, Prediction: 2622\n",
      "Iteration: 93, Prediction: 4172\n",
      "Iteration: 94, Prediction: 322\n",
      "Iteration: 95, Prediction: 574\n",
      "Iteration: 96, Prediction: 4628\n",
      "Iteration: 97, Prediction: 2368\n",
      "Iteration: 98, Prediction: 2622\n",
      "Iteration: 99, Prediction: 2716\n",
      "Iteration: 100, Prediction: 4172\n",
      "Iteration: 101, Prediction: 327\n",
      "Iteration: 102, Prediction: 567\n",
      "Iteration: 103, Prediction: 4628\n",
      "Iteration: 104, Prediction: 2370\n",
      "Iteration: 105, Prediction: 2622\n",
      "Iteration: 106, Prediction: 4172\n",
      "Iteration: 107, Prediction: 322\n",
      "Iteration: 108, Prediction: 574\n",
      "Iteration: 109, Prediction: 4628\n",
      "Iteration: 110, Prediction: 2368\n",
      "Iteration: 111, Prediction: 2622\n",
      "Iteration: 112, Prediction: 2716\n",
      "Iteration: 113, Prediction: 4172\n",
      "Iteration: 114, Prediction: 327\n",
      "Iteration: 115, Prediction: 567\n",
      "Iteration: 116, Prediction: 4628\n",
      "Iteration: 117, Prediction: 2370\n",
      "Iteration: 118, Prediction: 2622\n",
      "Iteration: 119, Prediction: 4172\n",
      "Iteration: 120, Prediction: 322\n",
      "Iteration: 121, Prediction: 574\n",
      "Iteration: 122, Prediction: 4628\n",
      "Iteration: 123, Prediction: 2368\n",
      "Iteration: 124, Prediction: 2622\n",
      "Iteration: 125, Prediction: 2716\n",
      "Iteration: 126, Prediction: 4172\n",
      "Iteration: 127, Prediction: 327\n",
      "Iteration: 128, Prediction: 567\n",
      "Iteration: 129, Prediction: 4628\n",
      "Iteration: 130, Prediction: 2370\n",
      "Iteration: 131, Prediction: 2622\n",
      "Iteration: 132, Prediction: 4172\n",
      "Iteration: 133, Prediction: 322\n",
      "Iteration: 134, Prediction: 574\n",
      "Iteration: 135, Prediction: 4628\n",
      "Iteration: 136, Prediction: 2368\n",
      "Iteration: 137, Prediction: 2622\n",
      "Iteration: 138, Prediction: 2716\n",
      "Iteration: 139, Prediction: 4172\n",
      "Iteration: 140, Prediction: 327\n",
      "Iteration: 141, Prediction: 567\n",
      "Iteration: 142, Prediction: 4628\n",
      "Iteration: 143, Prediction: 2370\n",
      "Iteration: 144, Prediction: 2622\n",
      "Iteration: 145, Prediction: 4172\n",
      "Iteration: 146, Prediction: 322\n",
      "Iteration: 147, Prediction: 574\n",
      "Iteration: 148, Prediction: 4628\n",
      "Iteration: 149, Prediction: 2368\n",
      "Iteration: 150, Prediction: 2622\n",
      "Iteration: 151, Prediction: 2716\n",
      "Iteration: 152, Prediction: 4172\n",
      "Iteration: 153, Prediction: 327\n",
      "Iteration: 154, Prediction: 567\n",
      "Iteration: 155, Prediction: 4628\n",
      "Iteration: 156, Prediction: 2370\n",
      "Iteration: 157, Prediction: 2622\n",
      "Iteration: 158, Prediction: 4172\n",
      "Iteration: 159, Prediction: 322\n",
      "Iteration: 160, Prediction: 574\n",
      "Iteration: 161, Prediction: 4628\n",
      "Iteration: 162, Prediction: 2368\n",
      "Iteration: 163, Prediction: 2622\n",
      "Iteration: 164, Prediction: 2716\n",
      "Iteration: 165, Prediction: 4172\n",
      "Iteration: 166, Prediction: 327\n",
      "Iteration: 167, Prediction: 567\n",
      "Iteration: 168, Prediction: 4628\n",
      "Iteration: 169, Prediction: 2370\n",
      "Iteration: 170, Prediction: 2622\n",
      "Iteration: 171, Prediction: 4172\n",
      "Iteration: 172, Prediction: 322\n",
      "Iteration: 173, Prediction: 574\n",
      "Iteration: 174, Prediction: 4628\n",
      "Iteration: 175, Prediction: 2368\n",
      "Iteration: 176, Prediction: 2622\n",
      "Iteration: 177, Prediction: 2716\n",
      "Iteration: 178, Prediction: 4172\n",
      "Iteration: 179, Prediction: 327\n",
      "Iteration: 180, Prediction: 567\n",
      "Iteration: 181, Prediction: 4628\n",
      "Iteration: 182, Prediction: 2370\n",
      "Iteration: 183, Prediction: 2622\n",
      "Iteration: 184, Prediction: 4172\n",
      "Iteration: 185, Prediction: 322\n",
      "Iteration: 186, Prediction: 574\n",
      "Iteration: 187, Prediction: 4628\n",
      "Iteration: 188, Prediction: 2368\n",
      "Iteration: 189, Prediction: 2622\n",
      "Iteration: 190, Prediction: 2716\n",
      "Iteration: 191, Prediction: 4172\n",
      "Iteration: 192, Prediction: 327\n",
      "Iteration: 193, Prediction: 567\n",
      "Iteration: 194, Prediction: 4628\n",
      "Iteration: 195, Prediction: 2370\n",
      "Iteration: 196, Prediction: 2622\n",
      "Iteration: 197, Prediction: 4172\n",
      "Iteration: 198, Prediction: 322\n",
      "Iteration: 199, Prediction: 574\n",
      "Iteration: 200, Prediction: 4628\n",
      "Iteration: 201, Prediction: 2368\n",
      "Iteration: 202, Prediction: 2622\n",
      "Iteration: 203, Prediction: 2716\n",
      "Iteration: 204, Prediction: 4172\n",
      "Iteration: 205, Prediction: 327\n",
      "Iteration: 206, Prediction: 567\n",
      "Iteration: 207, Prediction: 4628\n",
      "Iteration: 208, Prediction: 2370\n",
      "Iteration: 209, Prediction: 2622\n",
      "Iteration: 210, Prediction: 4172\n",
      "Iteration: 211, Prediction: 322\n",
      "Iteration: 212, Prediction: 574\n",
      "Iteration: 213, Prediction: 4628\n",
      "Iteration: 214, Prediction: 2368\n",
      "Iteration: 215, Prediction: 2622\n",
      "Iteration: 216, Prediction: 2716\n",
      "Iteration: 217, Prediction: 4172\n",
      "Iteration: 218, Prediction: 327\n",
      "Iteration: 219, Prediction: 567\n",
      "Iteration: 220, Prediction: 4628\n",
      "Iteration: 221, Prediction: 2370\n",
      "Iteration: 222, Prediction: 2622\n",
      "Iteration: 223, Prediction: 4172\n",
      "Iteration: 224, Prediction: 322\n",
      "Iteration: 225, Prediction: 574\n",
      "Iteration: 226, Prediction: 4628\n",
      "Iteration: 227, Prediction: 2368\n",
      "Iteration: 228, Prediction: 2622\n",
      "Iteration: 229, Prediction: 2716\n",
      "Iteration: 230, Prediction: 4172\n",
      "Iteration: 231, Prediction: 327\n",
      "Iteration: 232, Prediction: 567\n",
      "Iteration: 233, Prediction: 4628\n",
      "Iteration: 234, Prediction: 2370\n",
      "Iteration: 235, Prediction: 2622\n",
      "Iteration: 236, Prediction: 4172\n",
      "Iteration: 237, Prediction: 322\n",
      "Iteration: 238, Prediction: 574\n",
      "Iteration: 239, Prediction: 4628\n",
      "Iteration: 240, Prediction: 2368\n",
      "Iteration: 241, Prediction: 2622\n",
      "Iteration: 242, Prediction: 2716\n",
      "Iteration: 243, Prediction: 4172\n",
      "Iteration: 244, Prediction: 327\n",
      "Iteration: 245, Prediction: 567\n",
      "Iteration: 246, Prediction: 4628\n",
      "Iteration: 247, Prediction: 2370\n",
      "Iteration: 248, Prediction: 2622\n",
      "Iteration: 249, Prediction: 4172\n",
      "Iteration: 250, Prediction: 322\n",
      "Iteration: 251, Prediction: 574\n",
      "Iteration: 252, Prediction: 4628\n",
      "Iteration: 253, Prediction: 2368\n",
      "Iteration: 254, Prediction: 2622\n",
      "Iteration: 255, Prediction: 2716\n",
      "Iteration: 256, Prediction: 4172\n",
      "Iteration: 257, Prediction: 327\n",
      "Iteration: 258, Prediction: 567\n",
      "Iteration: 259, Prediction: 4628\n",
      "Iteration: 260, Prediction: 2370\n",
      "Iteration: 261, Prediction: 2622\n",
      "Iteration: 262, Prediction: 4172\n",
      "Iteration: 263, Prediction: 322\n",
      "Iteration: 264, Prediction: 574\n",
      "Iteration: 265, Prediction: 4628\n",
      "Iteration: 266, Prediction: 2368\n",
      "Iteration: 267, Prediction: 2622\n",
      "Iteration: 268, Prediction: 2716\n",
      "Iteration: 269, Prediction: 4172\n",
      "Iteration: 270, Prediction: 327\n",
      "Iteration: 271, Prediction: 567\n",
      "Iteration: 272, Prediction: 4628\n",
      "Iteration: 273, Prediction: 2370\n",
      "Iteration: 274, Prediction: 2622\n",
      "Iteration: 275, Prediction: 4172\n",
      "Iteration: 276, Prediction: 322\n",
      "Iteration: 277, Prediction: 574\n",
      "Iteration: 278, Prediction: 4628\n",
      "Iteration: 279, Prediction: 2368\n",
      "Iteration: 280, Prediction: 2622\n",
      "Iteration: 281, Prediction: 2716\n",
      "Iteration: 282, Prediction: 4172\n",
      "Iteration: 283, Prediction: 327\n",
      "Iteration: 284, Prediction: 567\n",
      "Iteration: 285, Prediction: 4628\n",
      "Iteration: 286, Prediction: 2370\n",
      "Iteration: 287, Prediction: 2622\n",
      "Iteration: 288, Prediction: 4172\n",
      "Iteration: 289, Prediction: 322\n",
      "Iteration: 290, Prediction: 574\n",
      "Iteration: 291, Prediction: 4628\n",
      "Iteration: 292, Prediction: 2368\n",
      "Iteration: 293, Prediction: 2622\n",
      "Iteration: 294, Prediction: 2716\n",
      "Iteration: 295, Prediction: 4172\n",
      "Iteration: 296, Prediction: 327\n",
      "Iteration: 297, Prediction: 567\n",
      "Iteration: 298, Prediction: 4628\n",
      "Iteration: 299, Prediction: 2370\n",
      "Iteration: 300, Prediction: 2622\n",
      "Iteration: 301, Prediction: 4172\n",
      "Iteration: 302, Prediction: 322\n",
      "Iteration: 303, Prediction: 574\n",
      "Iteration: 304, Prediction: 4628\n",
      "Iteration: 305, Prediction: 2368\n",
      "Iteration: 306, Prediction: 2622\n",
      "Iteration: 307, Prediction: 2716\n",
      "Iteration: 308, Prediction: 4172\n",
      "Iteration: 309, Prediction: 327\n",
      "Iteration: 310, Prediction: 567\n",
      "Iteration: 311, Prediction: 4628\n",
      "Iteration: 312, Prediction: 2370\n",
      "Iteration: 313, Prediction: 2622\n",
      "Iteration: 314, Prediction: 4172\n",
      "Iteration: 315, Prediction: 322\n",
      "Iteration: 316, Prediction: 574\n",
      "Iteration: 317, Prediction: 4628\n",
      "Iteration: 318, Prediction: 2368\n",
      "Iteration: 319, Prediction: 2622\n",
      "Iteration: 320, Prediction: 2716\n",
      "Iteration: 321, Prediction: 4172\n",
      "Iteration: 322, Prediction: 327\n",
      "Iteration: 323, Prediction: 567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 324, Prediction: 4628\n",
      "Iteration: 325, Prediction: 2370\n",
      "Iteration: 326, Prediction: 2622\n",
      "Iteration: 327, Prediction: 4172\n",
      "Iteration: 328, Prediction: 322\n",
      "Iteration: 329, Prediction: 574\n",
      "Iteration: 330, Prediction: 4628\n",
      "Iteration: 331, Prediction: 2368\n",
      "Iteration: 332, Prediction: 2622\n",
      "Iteration: 333, Prediction: 2716\n",
      "Iteration: 334, Prediction: 4172\n",
      "Iteration: 335, Prediction: 327\n",
      "Iteration: 336, Prediction: 567\n",
      "Iteration: 337, Prediction: 4628\n",
      "Iteration: 338, Prediction: 2370\n",
      "Iteration: 339, Prediction: 2622\n",
      "Iteration: 340, Prediction: 4172\n",
      "Iteration: 341, Prediction: 322\n",
      "Iteration: 342, Prediction: 574\n",
      "Iteration: 343, Prediction: 4628\n",
      "Iteration: 344, Prediction: 2368\n",
      "Iteration: 345, Prediction: 2622\n",
      "Iteration: 346, Prediction: 2716\n",
      "Iteration: 347, Prediction: 4172\n",
      "Iteration: 348, Prediction: 327\n",
      "Iteration: 349, Prediction: 567\n",
      "Iteration: 350, Prediction: 4628\n",
      "Iteration: 351, Prediction: 2370\n",
      "Iteration: 352, Prediction: 2622\n",
      "Iteration: 353, Prediction: 4172\n",
      "Iteration: 354, Prediction: 322\n",
      "Iteration: 355, Prediction: 574\n",
      "Iteration: 356, Prediction: 4628\n",
      "Iteration: 357, Prediction: 2368\n",
      "Iteration: 358, Prediction: 2622\n",
      "Iteration: 359, Prediction: 2716\n",
      "Iteration: 360, Prediction: 4172\n",
      "Iteration: 361, Prediction: 327\n",
      "Iteration: 362, Prediction: 567\n",
      "Iteration: 363, Prediction: 4628\n",
      "Iteration: 364, Prediction: 2370\n",
      "Iteration: 365, Prediction: 2622\n",
      "Iteration: 366, Prediction: 4172\n",
      "Iteration: 367, Prediction: 322\n",
      "Iteration: 368, Prediction: 574\n",
      "Iteration: 369, Prediction: 4628\n",
      "Iteration: 370, Prediction: 2368\n",
      "Iteration: 371, Prediction: 2622\n",
      "Iteration: 372, Prediction: 2716\n",
      "Iteration: 373, Prediction: 4172\n",
      "Iteration: 374, Prediction: 327\n",
      "Iteration: 375, Prediction: 567\n",
      "Iteration: 376, Prediction: 4628\n",
      "Iteration: 377, Prediction: 2370\n",
      "Iteration: 378, Prediction: 2622\n",
      "Iteration: 379, Prediction: 4172\n",
      "Iteration: 380, Prediction: 322\n",
      "Iteration: 381, Prediction: 574\n",
      "Iteration: 382, Prediction: 4628\n",
      "Iteration: 383, Prediction: 2368\n",
      "Iteration: 384, Prediction: 2622\n",
      "Iteration: 385, Prediction: 2716\n",
      "Iteration: 386, Prediction: 4172\n",
      "Iteration: 387, Prediction: 327\n",
      "Iteration: 388, Prediction: 567\n",
      "Iteration: 389, Prediction: 4628\n",
      "Iteration: 390, Prediction: 2370\n",
      "Iteration: 391, Prediction: 2622\n",
      "Iteration: 392, Prediction: 4172\n",
      "Iteration: 393, Prediction: 322\n",
      "Iteration: 394, Prediction: 574\n",
      "Iteration: 395, Prediction: 4628\n",
      "Iteration: 396, Prediction: 2368\n",
      "Iteration: 397, Prediction: 2622\n",
      "Iteration: 398, Prediction: 2716\n",
      "Iteration: 399, Prediction: 4172\n",
      "Iteration: 400, Prediction: 327\n",
      "Iteration: 401, Prediction: 567\n",
      "Iteration: 402, Prediction: 4628\n",
      "Iteration: 403, Prediction: 2370\n",
      "Iteration: 404, Prediction: 2622\n",
      "Iteration: 405, Prediction: 4172\n",
      "Iteration: 406, Prediction: 322\n",
      "Iteration: 407, Prediction: 574\n",
      "Iteration: 408, Prediction: 4628\n",
      "Iteration: 409, Prediction: 2368\n",
      "Iteration: 410, Prediction: 2622\n",
      "Iteration: 411, Prediction: 2716\n",
      "Iteration: 412, Prediction: 4172\n",
      "Iteration: 413, Prediction: 327\n",
      "Iteration: 414, Prediction: 567\n",
      "Iteration: 415, Prediction: 4628\n",
      "Iteration: 416, Prediction: 2370\n",
      "Iteration: 417, Prediction: 2622\n",
      "Iteration: 418, Prediction: 4172\n",
      "Iteration: 419, Prediction: 322\n",
      "Iteration: 420, Prediction: 574\n",
      "Iteration: 421, Prediction: 4628\n",
      "Iteration: 422, Prediction: 2368\n",
      "Iteration: 423, Prediction: 2622\n",
      "Iteration: 424, Prediction: 2716\n",
      "Iteration: 425, Prediction: 4172\n",
      "Iteration: 426, Prediction: 327\n",
      "Iteration: 427, Prediction: 567\n",
      "Iteration: 428, Prediction: 4628\n",
      "Iteration: 429, Prediction: 2370\n",
      "Iteration: 430, Prediction: 2622\n",
      "Iteration: 431, Prediction: 4172\n",
      "Iteration: 432, Prediction: 322\n",
      "Iteration: 433, Prediction: 574\n",
      "Iteration: 434, Prediction: 4628\n",
      "Iteration: 435, Prediction: 2368\n",
      "Iteration: 436, Prediction: 2622\n",
      "Iteration: 437, Prediction: 2716\n",
      "Iteration: 438, Prediction: 4172\n",
      "Iteration: 439, Prediction: 327\n",
      "Iteration: 440, Prediction: 567\n",
      "Iteration: 441, Prediction: 4628\n",
      "Iteration: 442, Prediction: 2370\n",
      "Iteration: 443, Prediction: 2622\n",
      "Iteration: 444, Prediction: 4172\n",
      "Iteration: 445, Prediction: 322\n",
      "Iteration: 446, Prediction: 574\n",
      "Iteration: 447, Prediction: 4628\n",
      "Iteration: 448, Prediction: 2368\n",
      "Iteration: 449, Prediction: 2622\n",
      "Iteration: 450, Prediction: 2716\n",
      "Iteration: 451, Prediction: 4172\n",
      "Iteration: 452, Prediction: 327\n",
      "Iteration: 453, Prediction: 567\n",
      "Iteration: 454, Prediction: 4628\n",
      "Iteration: 455, Prediction: 2370\n",
      "Iteration: 456, Prediction: 2622\n",
      "Iteration: 457, Prediction: 4172\n",
      "Iteration: 458, Prediction: 322\n",
      "Iteration: 459, Prediction: 574\n",
      "Iteration: 460, Prediction: 4628\n",
      "Iteration: 461, Prediction: 2368\n",
      "Iteration: 462, Prediction: 2622\n",
      "Iteration: 463, Prediction: 2716\n",
      "Iteration: 464, Prediction: 4172\n",
      "Iteration: 465, Prediction: 327\n",
      "Iteration: 466, Prediction: 567\n",
      "Iteration: 467, Prediction: 4628\n",
      "Iteration: 468, Prediction: 2370\n",
      "Iteration: 469, Prediction: 2622\n",
      "Iteration: 470, Prediction: 4172\n",
      "Iteration: 471, Prediction: 322\n",
      "Iteration: 472, Prediction: 574\n",
      "Iteration: 473, Prediction: 4628\n",
      "Iteration: 474, Prediction: 2368\n",
      "Iteration: 475, Prediction: 2622\n",
      "Iteration: 476, Prediction: 2716\n",
      "Iteration: 477, Prediction: 4172\n",
      "Iteration: 478, Prediction: 327\n",
      "Iteration: 479, Prediction: 567\n",
      "Iteration: 480, Prediction: 4628\n",
      "Iteration: 481, Prediction: 2370\n",
      "Iteration: 482, Prediction: 2622\n",
      "Iteration: 483, Prediction: 4172\n",
      "Iteration: 484, Prediction: 322\n",
      "Iteration: 485, Prediction: 574\n",
      "Iteration: 486, Prediction: 4628\n",
      "Iteration: 487, Prediction: 2368\n",
      "Iteration: 488, Prediction: 2622\n",
      "Iteration: 489, Prediction: 2716\n",
      "Iteration: 490, Prediction: 4172\n",
      "Iteration: 491, Prediction: 327\n",
      "Iteration: 492, Prediction: 567\n",
      "Iteration: 493, Prediction: 4628\n",
      "Iteration: 494, Prediction: 2370\n",
      "Iteration: 495, Prediction: 2622\n",
      "Iteration: 496, Prediction: 4172\n",
      "Iteration: 497, Prediction: 322\n",
      "Iteration: 498, Prediction: 574\n",
      "Iteration: 499, Prediction: 4628\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"../models/song.mod\"))\n",
    "learner.eval()\n",
    "gen_seq = learner.map_inference(song[:100])\n",
    "midi = loader.detokenize(np.array(song[:100] + gen_seq))\n",
    "midi.write('test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing Learner \"\"\"\n",
    "print_every = 10\n",
    "total_epochs = 2900\n",
    "print_loss_total = 0\n",
    "batch_size = 2\n",
    "startTime = time.time()\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    supports = eps.get_episode().support[0]\n",
    "    input_variables = []\n",
    "    original_sequences = []\n",
    "    \n",
    "    for i, support in enumerate(supports):\n",
    "        trunc_seq = [SOS_TOKEN] + list(support) + [EOS_TOKEN]\n",
    "        original_sequences.append(trunc_seq)\n",
    "        seq_length = len(trunc_seq)\n",
    "\n",
    "        trunc_seq = torch.from_numpy(np.array(one_hot_embeddings[trunc_seq])) # This is really time consuming\n",
    "\n",
    "        trunc_seq = trunc_seq.view(seq_length, vocabulary_size)\n",
    "        trunc_seq = Variable(trunc_seq)\n",
    "        input_variables.append(trunc_seq)\n",
    "    \n",
    "    original_sequences = np.array(original_sequences, dtype=np.int64)\n",
    "\n",
    "    for batch in range(len(input_variables)//batch_size):\n",
    "        # lstm input is (seq_len, batch_size, vocab_size)\n",
    "        start, end = batch*batch_size, (batch+1)*batch_size\n",
    "        sequences = torch.stack(input_variables[start:end]).transpose(0,1)\n",
    "        numbered_seqs = torch.stack(Variable(torch.from_numpy(original_sequences[start:end]))).transpose(0,1)\n",
    "        if use_cuda:\n",
    "            sequences = sequences.cuda()\n",
    "            numbered_seqs = numbered_seqs.cuda()\n",
    "        loss = learner.train(sequences, numbered_seqs, hidden=None)\n",
    "        print_loss_total += loss\n",
    "        del sequences, numbered_seqs\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(startTime, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg))\n",
    "        torch.save(learner.state_dict(), '../models/baseline_'+str(epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "def memReport():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or torch.is_tensor(obj.data):\n",
    "                print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()\n",
    "memReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_files = ['bach_846.mid', 'mz_311_1.mid', 'rac_op3_2.mid']\n",
    "input_variables = []\n",
    "original_sequences = []\n",
    "\n",
    "for index in range(0, 290):\n",
    "    supports = eps.get_episode().support[0]\n",
    "    \n",
    "    for i, support in enumerate(supports):\n",
    "        trunc_seq = [SOS_TOKEN] + list(support) + [EOS_TOKEN]\n",
    "        original_sequences.append(trunc_seq)\n",
    "        seq_length = len(trunc_seq)\n",
    "\n",
    "        trunc_seq = torch.from_numpy(np.array(one_hot_embeddings[trunc_seq])) # This is really time consuming\n",
    "\n",
    "        trunc_seq = trunc_seq.view(seq_length, vocabulary_size)\n",
    "        trunc_seq = Variable(trunc_seq)\n",
    "        if use_cuda:\n",
    "            trunc_seq = trunc_seq.cuda()\n",
    "        input_variables.append(trunc_seq)\n",
    "    \n",
    "original_sequences = np.array(original_sequences, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load_state_dict(torch.load('../models/baseline_'+epoch+'_'+print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[1 x 1 x 4711]' is invalid for input with 1 elements at /opt/conda/conda-bld/pytorch_1518244507981/work/torch/lib/TH/THStorage.c:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-85f4b5ea9bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msong_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msong_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# First dimension is batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwhole_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msong_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmidi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-12c914d1f17a>\u001b[0m in \u001b[0;36mmap_inference\u001b[0;34m(self, sequence, hidden, embeddings, max_length)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_control\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_control\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Gets hidden for next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: size '[1 x 1 x 4711]' is invalid for input with 1 elements at /opt/conda/conda-bld/pytorch_1518244507981/work/torch/lib/TH/THStorage.c:41"
     ]
    }
   ],
   "source": [
    "song_index = 0\n",
    "test_seq = input_variables[song_index][:,1:101] # First dimension is batch\n",
    "out_seq = learner.map_inference(test_seq, hidden=None)\n",
    "whole_seq = original_sequences[song_index][0:100].tolist() + out_seq\n",
    "midi = loader.detokenize(np.array(whole_seq))\n",
    "midi.write('test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.cuda.DoubleTensor of size 1x2x500 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_hidden = learner.encoder.initHidden(None).squeeze()\n",
    "print(torch.stack([test_hidden, test_hidden]).unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
