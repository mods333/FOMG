{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pickle\n",
    "from loaders import *\n",
    "from episode import *\n",
    "from dataset import *\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Some global variables \"\"\"\n",
    "_loader = Loader(502) # 500 + SOS + EOS\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Is the tokenizer 1 indexed?\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2 # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "\n",
    "encoding_size = 500\n",
    "one_hot_embeddings = np.eye(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.double()\n",
    "    loss = losses.sum() / length.double().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        _, hidden_out = self.lstm(input, hidden) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output[0], hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 learning_rate,\n",
    "                 embeddings=one_hot_embeddings):\n",
    "        super(Learner,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = EncoderLSTM(input_size, hidden_size)\n",
    "        self.decoder = DecoderLSTM(input_size, hidden_size, output_size)\n",
    "        self.encoder_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "        self.decoder_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.encoder = self.encoder.cuda()\n",
    "            self.decoder = self.decoder.cuda()\n",
    "            self.criterion = self.criterion.cuda()\n",
    "        \n",
    "    def forward(self, token_seqs):\n",
    "        loss = 0\n",
    "        batch_size = len(token_seqs)\n",
    "        seq_len = len(token_seqs[0])\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_hidden = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        encoder_output = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            encoder_output = encoder_output.cuda()\n",
    "        \n",
    "        hidden = (encoder_output, encoder_hidden)\n",
    "        for i in np.arange(seq_len-1, 0, -1):\n",
    "            token_batch = np.array(self.embeddings[token_seqs[:, i]])\n",
    "            encoder_input = Variable(torch.from_numpy(token_batch)).view(1, batch_size, -1).double()\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            #print(\"encoder_input: %d\" % (np.where(encoder_input.data==1)[2][0]))\n",
    "            hidden = self.encoder(encoder_input, hidden)\n",
    "        encoder_output, encoder_hidden = hidden\n",
    "            \n",
    "        token_batch = np.array(self.embeddings[[SOS_TOKEN]*batch_size])\n",
    "        decoder_input = Variable(torch.from_numpy(token_batch)).double()\n",
    "        decoder_output = Variable(self.decoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            decoder_output = decoder_output.cuda()\n",
    "        \n",
    "        hidden = (decoder_output, encoder_hidden)\n",
    "        for i in range(seq_len+1):\n",
    "            decoder_input = decoder_input.squeeze().view(1, batch_size, -1)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            #print(\"decoder_input: %d\" % (np.where(decoder_input.data==1)[2][0]))\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            #print(\"prediction: %d\" % (int(decoder_output.topk(1)[1])))\n",
    "            if i < seq_len:\n",
    "                seq_var = token_seqs[:, i]\n",
    "            else:\n",
    "                seq_var = [EOS_TOKEN]*batch_size\n",
    "                \n",
    "            target = Variable(torch.from_numpy(np.array(seq_var))).long()\n",
    "            target = target.cuda() if use_cuda else target\n",
    "            loss += self.criterion(decoder_output, target)\n",
    "                \n",
    "            # Teacher forcing\n",
    "            decoder_input = Variable(torch.from_numpy(np.array(self.embeddings[seq_var]))).double()\n",
    "            \n",
    "        loss = torch.sum(loss)/batch_size\n",
    "        loss.backward()\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "        return loss.data[0] / seq_len\n",
    "    \n",
    "    def map_inference(self, token_seqs):\n",
    "        batch_size = len(token_seqs)\n",
    "        seq_len = len(token_seqs[0])\n",
    "        encoder_hidden = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        encoder_output = Variable(self.encoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            encoder_hidden = encoder_hidden.cuda()\n",
    "            encoder_output = encoder_output.cuda()\n",
    "        \n",
    "        hidden = (encoder_output, encoder_hidden)\n",
    "        for i in np.arange(seq_len-1, 0, -1):\n",
    "            token_batch = np.array(self.embeddings[token_seqs[:, i]])\n",
    "            encoder_input = Variable(torch.from_numpy(token_batch)).view(1, batch_size, -1).double()\n",
    "            encoder_input = encoder_input.cuda() if use_cuda else encoder_input\n",
    "            hidden = self.encoder(encoder_input, hidden)\n",
    "        \n",
    "        encoder_output, encoder_hidden = hidden\n",
    "            \n",
    "        token_batch = np.array(self.embeddings[[SOS_TOKEN]*batch_size])\n",
    "        decoder_output = Variable(self.decoder.initHidden(batch_size)).double()\n",
    "        if use_cuda:\n",
    "            decoder_output = decoder_output.cuda()\n",
    "        \n",
    "        hidden = (decoder_output, encoder_hidden)\n",
    "\n",
    "        pred_seqs = None\n",
    "        for i in range(500):\n",
    "            decoder_input = Variable(torch.from_numpy(token_batch)).double()\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            decoder_input = decoder_input.squeeze().view(1, batch_size, -1)\n",
    "            decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            #print(\"Iteration: %d, Prediction: %d\" % (i, token))\n",
    "            if pred_seqs is None:\n",
    "                pred_seqs = topi.cpu().numpy()\n",
    "            else:\n",
    "                pred_seqs = np.concatenate((pred_seqs, topi.cpu().numpy()), axis=1)\n",
    "            token_batch = np.array(self.embeddings[topi])\n",
    "            \n",
    "        return pred_seqs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(vocabulary_size, \n",
    "              encoding_size, \n",
    "              vocabulary_size,\n",
    "              learning_rate=0.01)\n",
    "\n",
    "learner = learner.cuda() if use_cuda else learner\n",
    "eps = load_sampler_from_config(\"../src/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jimi Hendrix hearmytrainacomin.mid\n",
      "Jimi Hendrix gypsyeyes.mid\n",
      "Jimi Hendrix thestarspangledbanner.mid\n",
      "Jimi Hendrix electricladyland.mid\n",
      "Jimi Hendrix stonefree.mid\n",
      "Jimi Hendrix messagetolove.mid\n",
      "Jimi Hendrix drifting.mid\n",
      "Jimi Hendrix thewindcriesmary.mid\n",
      "Jimi Hendrix haveyoueverbeen.mid\n",
      "Jimi Hendrix upfromtheskies.mid\n",
      "Jimi Hendrix spanishcastlemagicsolo(woodstock).mid\n"
     ]
    }
   ],
   "source": [
    "songs = np.concatenate(eps.get_episode().support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4248,  551, 4708, ...,  568,  679, 4647],\n",
       "       [4248,  551, 4708, ...,  568,  679, 4647],\n",
       "       [4248,  551, 4708, ...,  568,  679, 4647],\n",
       "       ...,\n",
       "       [4248,  551, 4708, ...,  568,  679, 4647],\n",
       "       [4248,  551, 4708, ...,  568,  679, 4647],\n",
       "       [4248,  551, 4708, ...,  568,  679, 4647]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 53s (- 3m 32s) (10 20%) 0.4734\n",
      "1m 46s (- 2m 40s) (20 40%) 0.3523\n",
      "2m 41s (- 1m 47s) (30 60%) 0.3376\n",
      "3m 38s (- 0m 54s) (40 80%) 0.3296\n",
      "4m 32s (- 0m 0s) (50 100%) 0.3253\n"
     ]
    }
   ],
   "source": [
    "print_every = 10\n",
    "total_epochs = 50\n",
    "print_loss_total = 0\n",
    "startTime = time.time()\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    #songs = np.concatenate(eps.get_episode().support)\n",
    "    loss = learner(songs)\n",
    "    print_loss_total += loss\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(startTime, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg))\n",
    "        #torch.save(learner.state_dict(), '../models/baseline_'+str(epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 10\n",
    "total_epochs = 2900\n",
    "print_loss_total = 0\n",
    "startTime = time.time()\n",
    "for epoch in range(1001, total_epochs+1):\n",
    "    songs = eps.get_episode().support[0]\n",
    "    for song in songs:\n",
    "        song = list(song)\n",
    "        loss = learner(song)\n",
    "        print_loss_total += loss\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(startTime, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg))\n",
    "        torch.save(learner.state_dict(), '../models/baseline_'+str(epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.state_dict(), \"../models/song.mod\")\n",
    "\n",
    "pickle.dump(songs[0], open(\"../models/song.mid\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.load_state_dict(torch.load(\"../models/song.mod\"))\n",
    "learner.eval()\n",
    "song = pickle.load(open(\"../models/song.mid\", \"rb\"))\n",
    "song = song.tolist()\n",
    "\n",
    "midi = loader.detokenize(np.array(song))\n",
    "midi.write('orig.mid')\n",
    "\n",
    "midi = loader.detokenize(np.array(song[:100]))\n",
    "midi.write('song.mid')\n",
    "\n",
    "gen_seq = learner.map_inference(np.array([song[:100]]))\n",
    "midi = loader.detokenize(np.array(song[:100] + gen_seq[0]))\n",
    "midi.write('pred.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load_state_dict(torch.load('../models/baseline_'+epoch+'_'+print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_index = 0\n",
    "test_seq = input_variables[song_index][:,1:101] # First dimension is batch\n",
    "out_seq = learner.map_inference(test_seq, hidden=None)\n",
    "whole_seq = original_sequences[song_index][0:100].tolist() + out_seq\n",
    "midi = loader.detokenize(np.array(whole_seq))\n",
    "midi.write('test.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
