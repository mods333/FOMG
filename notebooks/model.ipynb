{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "        \n",
    "    def forward(self, input, hidden_in):\n",
    "        _, hidden_out = self.lstm(input, hidden_in) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self, hidden):\n",
    "        \n",
    "        if hidden == None:\n",
    "            result = Variable(torch.zeros(1, 1, self.hidden_size)).double()\n",
    "            \n",
    "            if use_cuda:\n",
    "                result = result.cuda()\n",
    "            return result\n",
    "        \n",
    "        else:\n",
    "            return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        self.project = nn.Linear(4096, self.hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "            self.project = self.project.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze()\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.double()\n",
    "    loss = losses.sum() / length.double().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabulary_size, encoding_size):\n",
    "        \n",
    "        self.encoder EncoderLSTM(vocabulary_size, encoding_size)\n",
    "        self.decoder = DecoderLSTM(vocabulary_size, encoding_size, vocabulary_size)\n",
    "        self.hidden = None\n",
    "        self.teacher_forcing = 0.9\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        \n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "        hidden = self.hidden\n",
    "        \n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        seq_size = sequence.size()\n",
    "        batch_size = seq_size[1]\n",
    "        sequence_length = seq_size[0]\n",
    "        loss = 0\n",
    "        \n",
    "        encoder_hidden = hidden\n",
    "        \n",
    "        # Encoder is fed the flipped control sequence\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            encoder_input = sequence[index_control].unsqueeze(0) # (1, batch_size, vocab_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "        \n",
    "        # feed encoder_hidden\n",
    "        decoder_input = sequence[1].unsqueeze(0) # One after SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        predicted_note_index = 0\n",
    "        \n",
    "        # Prepare the results tensor\n",
    "        all_decoder_outputs = Variable(torch.zeros(*sequence.size())).double() # (seq_length, batch_size, vocab_size)\n",
    "        if use_cuda:\n",
    "            all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "        all_decoder_outputs[0] = decoder_input\n",
    "\n",
    "        for index_control in range(2, sequence_length):\n",
    "            # decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            if random.random() <= self.teacher_forcing:\n",
    "                decoder_input = sequence[index_control].unsqueeze(0)\n",
    "            else:\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                # This is the next input, without teacher forcing it's the predicted output\n",
    "                decoder_input = torch.stack([Variable(torch.DoubleTensor(embeddings[ni]))\n",
    "                                         for ni in topi.squeeze()]).unsqueeze(0)\n",
    "                if use_cuda:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "                    \n",
    "            # Save the decoder output\n",
    "            all_decoder_outputs[index_control] = decoder_output\n",
    "        \n",
    "        \n",
    "        seq_lens = Variable(torch.LongTensor(np.ones(batch_size, dtype=int)*sequence_length))\n",
    "        if use_cuda:\n",
    "            seq_lens = seq_lens.cuda()\n",
    "        loss = compute_loss(all_decoder_outputs.transpose(0,1).contiguous(),\n",
    "                        numbered_seq.transpose(0,1).contiguous(), \n",
    "                        seq_lens)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def map_inference(self, sequence, embeddings=one_hot_embeddings, max_length=500):\n",
    "        \"\"\" sequence has to be batch_size=1\"\"\"\n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "        \n",
    "        output_control_sequence = []\n",
    "    \n",
    "        # Encoder\n",
    "        encoder_hidden = self.hidden\n",
    "\n",
    "        sequence_length = sequence.size()[1]\n",
    "\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            encoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input\n",
    "\n",
    "        # This point we have last encoder_hidden, feed into decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = sequence[0][0]\n",
    "        predicted_control_index = SOS_TOKEN\n",
    "\n",
    "        cur_length = 0\n",
    "        while True:\n",
    "            decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # MAP inference\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            predicted_control_index = int(topi)\n",
    "            if predicted_control_index == EOS_TOKEN:\n",
    "                break\n",
    "            output_control_sequence.append(predicted_control_index)\n",
    "\n",
    "            # This is the next input\n",
    "            decoder_input = torch.from_numpy(embeddings[predicted_control_index])\n",
    "            decoder_input = Variable(decoder_input).double()\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "\n",
    "            cur_length += 1\n",
    "            if cur_length >= max_length:\n",
    "                break\n",
    "\n",
    "        return output_control_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
