{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from episode import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1  # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2  # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "\n",
    "encoding_size = 500\n",
    "one_hot_embeddings = np.eye(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner and MetaLearner Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "\n",
    "    def __init__(self, network_class, *args):\n",
    "\n",
    "        super(Learner, self).__init__()\n",
    "        # define the network for the learner and the meta-learner network\n",
    "        self.meta_net = network_class(*args)\n",
    "        self.learner_net = network_class(*args)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.learner_net.parameters(), 0.1)\n",
    "\n",
    "    def copy_theta(self):\n",
    "\n",
    "        # Ablation test -- set to 0s\n",
    "        self.learner_net.load_state_dict(self.meta_net.state_dict())\n",
    "\n",
    "    def forward(self, support_x, query_x, num_updates, support_cat, query_cat):\n",
    "\n",
    "        # Copy theta into theta'\n",
    "        self.copy_theta()\n",
    "\n",
    "        # update for several steps\n",
    "        for i in range(num_updates):\n",
    "            # forward and backward to update net_pi grad.\n",
    "            loss = self.learner_net(support_x, support_cat)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Find the loss on the query set\n",
    "        loss = self.learner_net(query_x, query_cat)\n",
    "\n",
    "        grads_pi = autograd.grad(\n",
    "            loss, self.learner_net.parameters(), retain_graph=True)\n",
    "\n",
    "        return loss, grads_pi, loss.data[0]\n",
    "\n",
    "    def net_forward(self, support_x, numbered_seq):\n",
    "\n",
    "        loss = self.meta_net(support_x, numbered_seq)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "\n",
    "    def __init__(self, network_class, network_args, k_shot, beta, num_updates):\n",
    "\n",
    "        super(MetaLearner, self).__init__()\n",
    "\n",
    "        self.k_shot = k_shot\n",
    "        self.beta = beta\n",
    "        self.num_updates = num_updates\n",
    "\n",
    "        # it will contains a learner class to learn on episodes and gather the loss together.\n",
    "        self.learner = Learner(network_class, *network_args)\n",
    "        # the optimizer is to update theta parameters, not theta_pi parameters.\n",
    "        self.optimizer = torch.optim.Adam(self.learner.parameters(), lr=beta)\n",
    "\n",
    "    def write_grads(self, dummy_loss, sum_grads_pi):\n",
    "        \"\"\"\n",
    "        write loss into learner.net, gradients come from sum_grads_pi.\n",
    "        Since the gradients info is not calculated by general backward, we need this function to write the right gradients\n",
    "        into theta network and update theta parameters as wished.\n",
    "        :param dummy_loss: dummy loss, nothing but to write our gradients by hook\n",
    "        :param sum_grads_pi: the summed gradients\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Register a hook on each parameter in the net that replaces the current dummy grad\n",
    "        # with our grads accumulated across the meta-batch\n",
    "        hooks = []\n",
    "\n",
    "        for i, v in enumerate(self.learner.parameters()):\n",
    "            def closure():\n",
    "                ii = i\n",
    "                return lambda grad: sum_grads_pi[ii]\n",
    "\n",
    "            # if you write: hooks.append( v.register_hook(lambda grad : sum_grads_pi[i]) )\n",
    "            # it will pop an ERROR, i don't know why?\n",
    "            hooks.append(v.register_hook(closure()))\n",
    "\n",
    "        # use our sumed gradients_pi to update the theta/net network,\n",
    "        # since our optimizer receive the self.net.parameters() only.\n",
    "        self.optimizer.zero_grad()\n",
    "        dummy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # if you do NOT remove the hook, the GPU memory will expode!!!\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "    def forward(self, support_x, query_x, support_cat, query_cat):\n",
    "\n",
    "        sum_grads_pi = None\n",
    "        # (T_i, seq_length, batch_size, vocab_size)\n",
    "        meta_batchsz = support_x.size(0)\n",
    "\n",
    "        # we do different learning task sequentially, not parallel.\n",
    "        accs = []\n",
    "        # for each task/episode.\n",
    "        for i in range(meta_batchsz):\n",
    "            # ASSUME QUERY SET IS ALWAYS SIZE 1\n",
    "            _, grad_pi, episode_acc = self.learner(\n",
    "                support_x[i], query_x[0], self.num_updates, support_cat[i], query_cat[0])\n",
    "            accs.append(episode_acc)\n",
    "            if sum_grads_pi is None:\n",
    "                sum_grads_pi = grad_pi\n",
    "            else:  # accumulate all gradients from different episode learner\n",
    "                sum_grads_pi = [torch.add(i, j)\n",
    "                                for i, j in zip(sum_grads_pi, grad_pi)]\n",
    "\n",
    "        # As we already have the grads to update\n",
    "        # We use a dummy forward / backward pass to get the correct grads into self.net\n",
    "        # the right grads will be updated by hook, ignoring backward.\n",
    "        # use hook mechnism to write sumed gradient into network.\n",
    "        # we need to update the theta/net network, we need a op from net network, so we call self.learner.net_forward\n",
    "        # to get the op from net network, since the loss from self.learner.forward will return loss from net_pi network.\n",
    "        dummy_loss = self.learner.net_forward(support_x[0], support_cat[0])\n",
    "        self.write_grads(dummy_loss, sum_grads_pi)\n",
    "\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "Just a simple LSTM encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "\n",
    "    def forward(self, input, hidden_in):\n",
    "        # encoder only outputs hidden\n",
    "        _, hidden_out = self.lstm(input, hidden_in)\n",
    "        return hidden_out\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "\n",
    "        result = Variable(torch.zeros(\n",
    "            1, batch_size, self.hidden_size)).double()\n",
    "\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # output = F.relu(input) \n",
    "        # FIXME: Don't think we need the RELU?  Input is one-hot, RELU does nothing. \n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze()\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(\n",
    "            1, batch_size, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.double()\n",
    "    loss = losses.sum() / length.double().sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, encoding_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = EncoderLSTM(vocabulary_size, encoding_size)\n",
    "        self.decoder = DecoderLSTM(\n",
    "            vocabulary_size, encoding_size, vocabulary_size)\n",
    "        self.teacher_forcing = 0.9\n",
    "\n",
    "    def forward(self, sequence, numbered_seq):\n",
    "\n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        seq_size = sequence.size()\n",
    "        batch_size = seq_size[1]\n",
    "        sequence_length = seq_size[0]\n",
    "        loss = 0\n",
    "\n",
    "        encoder_hidden = (encoder.initHidden(batch_size),\n",
    "                          encoder.initHidden(batch_size))\n",
    "\n",
    "        # Encoder is fed the flipped control sequence\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            encoder_input = sequence[index_control].unsqueeze(\n",
    "                0)  # (1, batch_size, vocab_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "\n",
    "        # feed encoder_hidden\n",
    "        decoder_input = sequence[0].unsqueeze(0)  # This is SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Prepare the results tensor\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        all_decoder_outputs = Variable(torch.zeros(*sequence.size())).double()\n",
    "        if use_cuda:\n",
    "            all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "        all_decoder_outputs[0] = decoder_input\n",
    "\n",
    "        for index_control in range(1, sequence_length):\n",
    "            # decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "\n",
    "            if random.random() <= self.teacher_forcing:\n",
    "                decoder_input = sequence[index_control].unsqueeze(0)\n",
    "            else:\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                # This is the next input, without teacher forcing it's the predicted output\n",
    "                decoder_input = torch.stack([Variable(torch.DoubleTensor(one_hot_embeddings[ni]))\n",
    "                                             for ni in topi.squeeze()]).unsqueeze(0)\n",
    "                if use_cuda:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Save the decoder output\n",
    "            all_decoder_outputs[index_control] = decoder_output\n",
    "\n",
    "        seq_lens = Variable(torch.LongTensor(\n",
    "            np.ones(batch_size, dtype=int)*sequence_length))\n",
    "        if use_cuda:\n",
    "            seq_lens = seq_lens.cuda()\n",
    "\n",
    "        loss = compute_loss(all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "                            numbered_seq.transpose(0, 1).contiguous(),\n",
    "                            seq_lens)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def map_inference(self, sequence, embeddings=one_hot_embeddings, max_length=500):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            sequence: (seq_length, batch, vocab_size)\n",
    "            output: [[seq1], ..., [seqN]] where N is number of batch\n",
    "        \"\"\"\n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        seq_size = sequence.size()\n",
    "        batch_size = seq_size[1]\n",
    "        sequence_length = seq_size[0]\n",
    "\n",
    "        encoder_hidden = (encoder.initHidden(batch_size),\n",
    "                          encoder.initHidden(batch_size))\n",
    "\n",
    "        # Encoder is fed the flipped control sequence\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            encoder_input = sequence[index_control].unsqueeze(\n",
    "                0)  # (1, batch_size, vocab_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "\n",
    "        # feed encoder_hidden\n",
    "        decoder_input = sequence[0].unsqueeze(0)  # This is SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        output_control_sequences = [[] for batch in range(batch_size)]\n",
    "        append_flag = [True for batch in range(batch_size)]\n",
    "        # Prepare the results tensor\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        index_control = 1\n",
    "        while True:\n",
    "            # decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "            next_input = []\n",
    "            for index, ni in enumerate(topi.squeeze()):\n",
    "                next_input.append(\n",
    "                    Variable(torch.DoubleTensor(one_hot_embeddings[ni])))\n",
    "                # If we hit an EOS, stop appending to that output sequence\n",
    "                if ni == EOS_TOKEN:\n",
    "                    append_flag[index] = False\n",
    "                if append_flag[index]:\n",
    "                    output_control_sequences[index].append(ni)\n",
    "\n",
    "            decoder_input = torch.stack(next_input).unsqueeze(0)\n",
    "\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "\n",
    "            index_control += 1\n",
    "            if index_control >= max_length:\n",
    "                break\n",
    "\n",
    "        return output_control_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '../data/small/'\n",
    "training_set = np.load(base_path + 'beethoven_brunomars_eminem_mozart.npy')\n",
    "train_size = len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS ONLY MANUALLY! \"\"\"\n",
    "meta_learner = MetaLearner(Model, (vocabulary_size, encoding_size), 1, 0.01, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert to one-hot \"\"\"\n",
    "training_one_hot = []\n",
    "training_categories = []\n",
    "for i in range(train_size):\n",
    "    training_categories.append(np.concatenate(([SOS_TOKEN], training_set[i], [EOS_TOKEN])))\n",
    "    training_one_hot.append(one_hot_embeddings[training_categories[i]])\n",
    "    \n",
    "training_one_hot = np.array(training_one_hot)\n",
    "training_categories = np.array(training_categories, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 502, 4711)\n",
      "(101, 502)\n"
     ]
    }
   ],
   "source": [
    "print(training_one_hot.shape)\n",
    "print(training_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 1\n",
    "TASK_BATCH_SIZE = 3 + 1 # 3 support + 1 query\n",
    "SEQ_LENGTH = 502\n",
    "\n",
    "print_every = 10\n",
    "check_every = 10\n",
    "print_loss_total = 0\n",
    "startTime = time.time()\n",
    "randomize_song_index = np.arange(train_size)\n",
    "for epoch in range(total_epochs):\n",
    "    # Randomize\n",
    "    np.random.shuffle(randomize_song_index)\n",
    "    training_one_hot = training_one_hot[randomize_song_index]\n",
    "    training_categories = training_categories[randomize_song_index]\n",
    "    \n",
    "    for task_batch in range(train_size // TASK_BATCH_SIZE):\n",
    "        batch_start = task_batch * TASK_BATCH_SIZE\n",
    "        batch_end = batch_start + TASK_BATCH_SIZE\n",
    "        batch = training_one_hot[batch_start:batch_end]\n",
    "        \n",
    "        support = batch[:-1]\n",
    "        support_cat = training_categories[batch_start:batch_end-1]\n",
    "        query = batch[-1:]\n",
    "        query_cat = training_categories[batch_end-1:batch_end]\n",
    "        \n",
    "        support = Variable(torch.from_numpy(support)).view(TASK_BATCH_SIZE-1, SEQ_LENGTH, 1, vocabulary_size)\n",
    "        support_cat = Variable(torch.from_numpy(support_cat)).view(TASK_BATCH_SIZE-1, SEQ_LENGTH, 1)\n",
    "        query = Variable(torch.from_numpy(query)).view(1, SEQ_LENGTH, 1, vocabulary_size)\n",
    "        query_cat = Variable(torch.from_numpy(query_cat)).view(1, SEQ_LENGTH, 1)\n",
    "        if use_cuda:\n",
    "            support = support.cuda()\n",
    "            support_cat = support_cat.cuda()\n",
    "            query = query.cuda()\n",
    "            query_cat = query_cat.cuda()\n",
    "        # support_x : (T_i, seq_length, batch_size, vocab_size)\n",
    "        # query_x : (T_i, seq_length, batch_size, vocab_size)\n",
    "        # num_seq : (T_i, seq_length, batch_size)\n",
    "        \n",
    "        loss = meta_learner(support, query, support_cat, query_cat)\n",
    "        print_loss_total += np.sum(np.array(loss))\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(startTime, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg)) \n",
    "        \n",
    "    if epoch % check_every == 0:\n",
    "        torch.save(learner.state_dict(), '../models/maml_'+str(epoch))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
