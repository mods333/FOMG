{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from episode import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1  # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2  # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "\n",
    "encoding_size = 500\n",
    "one_hot_embeddings = np.eye(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner and MetaLearner Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "\n",
    "    def __init__(self, network_class, *args):\n",
    "\n",
    "        super(Learner, self).__init__()\n",
    "        # define the network for the learner and the meta-learner network\n",
    "        self.meta_net = network_class(*args)\n",
    "        self.learner_net = network_class(*args)\n",
    "\n",
    "        self.optimizer = torch.optim.SGD(self.learner_net.parameters(), 0.001)\n",
    "\n",
    "    def copy_theta(self):\n",
    "\n",
    "        # Ablation test -- set to 0s\n",
    "        self.learner_net.load_state_dict(self.meta_net.state_dict())\n",
    "\n",
    "    def forward(self, support_x, query_x, num_updates, support_cat, query_cat):\n",
    "\n",
    "        # Copy theta into theta'\n",
    "        self.copy_theta()\n",
    "\n",
    "        # update for several steps\n",
    "        for i in range(num_updates):\n",
    "            # forward and backward to update net_pi grad.\n",
    "            loss = self.learner_net(support_x, support_cat)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Find the loss on the query set\n",
    "        loss = self.learner_net(query_x, query_cat)\n",
    "\n",
    "        grads_pi = autograd.grad(\n",
    "            loss, self.learner_net.parameters(), retain_graph=True)\n",
    "\n",
    "        return loss, grads_pi, loss.data[0]\n",
    "\n",
    "    def net_forward(self, support_x, numbered_seq):\n",
    "\n",
    "        loss = self.meta_net(support_x, numbered_seq)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "\n",
    "    def __init__(self, network_class, network_args, k_shot, beta, num_updates):\n",
    "\n",
    "        super(MetaLearner, self).__init__()\n",
    "\n",
    "        self.k_shot = k_shot\n",
    "        self.beta = beta\n",
    "        self.num_updates = num_updates\n",
    "\n",
    "        # it will contains a learner class to learn on episodes and gather the loss together.\n",
    "        self.learner = Learner(network_class, *network_args)\n",
    "        # the optimizer is to update theta parameters, not theta_pi parameters.\n",
    "        self.optimizer = torch.optim.Adam(self.learner.parameters(), lr=beta)\n",
    "\n",
    "    def write_grads(self, dummy_loss, sum_grads_pi):\n",
    "        \"\"\"\n",
    "        write loss into learner.net, gradients come from sum_grads_pi.\n",
    "        Since the gradients info is not calculated by general backward, we need this function to write the right gradients\n",
    "        into theta network and update theta parameters as wished.\n",
    "        :param dummy_loss: dummy loss, nothing but to write our gradients by hook\n",
    "        :param sum_grads_pi: the summed gradients\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Register a hook on each parameter in the net that replaces the current dummy grad\n",
    "        # with our grads accumulated across the meta-batch\n",
    "        hooks = []\n",
    "\n",
    "        for i, v in enumerate(self.learner.parameters()):\n",
    "            def closure():\n",
    "                ii = i\n",
    "                return lambda grad: sum_grads_pi[ii]\n",
    "\n",
    "            # if you write: hooks.append( v.register_hook(lambda grad : sum_grads_pi[i]) )\n",
    "            # it will pop an ERROR, i don't know why?\n",
    "            hooks.append(v.register_hook(closure()))\n",
    "\n",
    "        # use our sumed gradients_pi to update the theta/net network,\n",
    "        # since our optimizer receive the self.net.parameters() only.\n",
    "        self.optimizer.zero_grad()\n",
    "        dummy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # if you do NOT remove the hook, the GPU memory will expode!!!\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "    def forward(self, support_x, query_x, support_cat, query_cat):\n",
    "\n",
    "        sum_grads_pi = None\n",
    "        # (T_i, seq_length, batch_size, vocab_size)\n",
    "        meta_batchsz = support_x.size(0)\n",
    "\n",
    "        # we do different learning task sequentially, not parallel.\n",
    "        accs = []\n",
    "        # for each task/episode.\n",
    "        for i in range(meta_batchsz):\n",
    "            # ASSUME QUERY SET IS ALWAYS SIZE 1\n",
    "            _, grad_pi, episode_acc = self.learner(\n",
    "                support_x[i], query_x[0], self.num_updates, support_cat[i], query_cat[0])\n",
    "            accs.append(episode_acc)\n",
    "            if sum_grads_pi is None:\n",
    "                sum_grads_pi = grad_pi\n",
    "            else:  # accumulate all gradients from different episode learner\n",
    "                sum_grads_pi = [torch.add(i, j)\n",
    "                                for i, j in zip(sum_grads_pi, grad_pi)]\n",
    "\n",
    "        # As we already have the grads to update\n",
    "        # We use a dummy forward / backward pass to get the correct grads into self.net\n",
    "        # the right grads will be updated by hook, ignoring backward.\n",
    "        # use hook mechnism to write sumed gradient into network.\n",
    "        # we need to update the theta/net network, we need a op from net network, so we call self.learner.net_forward\n",
    "        # to get the op from net network, since the loss from self.learner.forward will return loss from net_pi network.\n",
    "        dummy_loss = self.learner.net_forward(support_x[0], support_cat[0])\n",
    "        self.write_grads(dummy_loss, sum_grads_pi)\n",
    "\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "Just a simple LSTM encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "\n",
    "    def forward(self, input, hidden_in):\n",
    "        # encoder only outputs hidden\n",
    "        _, hidden_out = self.lstm(input, hidden_in)\n",
    "        return hidden_out\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "\n",
    "        result = Variable(torch.zeros(\n",
    "            1, batch_size, self.hidden_size)).double()\n",
    "\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # output = F.relu(input) \n",
    "        # FIXME: Don't think we need the RELU?  Input is one-hot, RELU does nothing. \n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze()\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(\n",
    "            1, batch_size, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.double()\n",
    "    loss = losses.sum() / length.double().sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, encoding_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = EncoderLSTM(vocabulary_size, encoding_size)\n",
    "        self.decoder = DecoderLSTM(\n",
    "            vocabulary_size, encoding_size, vocabulary_size)\n",
    "        self.teacher_forcing = 0.9\n",
    "\n",
    "    def forward(self, sequence, numbered_seq):\n",
    "\n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        seq_size = sequence.size()\n",
    "        batch_size = seq_size[1]\n",
    "        sequence_length = seq_size[0]\n",
    "        loss = 0\n",
    "\n",
    "        encoder_hidden = (encoder.initHidden(batch_size),\n",
    "                          encoder.initHidden(batch_size))\n",
    "\n",
    "        # Encoder is fed the flipped control sequence\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            encoder_input = sequence[index_control].unsqueeze(\n",
    "                0)  # (1, batch_size, vocab_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "\n",
    "        # feed encoder_hidden\n",
    "        decoder_input = sequence[0].unsqueeze(0)  # This is SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Prepare the results tensor\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        all_decoder_outputs = Variable(torch.zeros(*sequence.size())).double()\n",
    "        if use_cuda:\n",
    "            all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "        all_decoder_outputs[0] = decoder_input\n",
    "\n",
    "        for index_control in range(1, sequence_length):\n",
    "            # decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "\n",
    "            if random.random() <= self.teacher_forcing:\n",
    "                decoder_input = sequence[index_control].unsqueeze(0)\n",
    "            else:\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                # This is the next input, without teacher forcing it's the predicted output\n",
    "                decoder_input = torch.stack([Variable(torch.DoubleTensor(one_hot_embeddings[ni]))\n",
    "                                             for ni in topi.squeeze()]).unsqueeze(0)\n",
    "                if use_cuda:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Save the decoder output\n",
    "            all_decoder_outputs[index_control] = decoder_output\n",
    "\n",
    "        seq_lens = Variable(torch.LongTensor(\n",
    "            np.ones(batch_size, dtype=int)*sequence_length))\n",
    "        if use_cuda:\n",
    "            seq_lens = seq_lens.cuda()\n",
    "\n",
    "        loss = compute_loss(all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "                            numbered_seq.transpose(0, 1).contiguous(),\n",
    "                            seq_lens)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def map_inference(self, sequence, embeddings=one_hot_embeddings, max_length=250):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            sequence: (seq_length, batch, vocab_size)\n",
    "            output: [[seq1], ..., [seqN]] where N is number of batch\n",
    "        \"\"\"\n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        seq_size = sequence.size()\n",
    "        batch_size = seq_size[1]\n",
    "        sequence_length = seq_size[0]\n",
    "\n",
    "        encoder_hidden = (encoder.initHidden(batch_size),\n",
    "                          encoder.initHidden(batch_size))\n",
    "\n",
    "        # Encoder is fed the flipped control sequence\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            encoder_input = sequence[index_control].unsqueeze(\n",
    "                0)  # (1, batch_size, vocab_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "\n",
    "        # feed encoder_hidden\n",
    "        decoder_input = sequence[0].unsqueeze(0)  # This is SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        output_control_sequences = [[] for batch in range(batch_size)]\n",
    "        append_flag = [True for batch in range(batch_size)]\n",
    "        # Prepare the results tensor\n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        index_control = 1\n",
    "        while True:\n",
    "            # decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "            next_input = []\n",
    "            for index, ni in enumerate(topi.squeeze()):\n",
    "                next_input.append(\n",
    "                    Variable(torch.DoubleTensor(one_hot_embeddings[ni])))\n",
    "                # If we hit an EOS, stop appending to that output sequence\n",
    "                if ni == EOS_TOKEN:\n",
    "                    append_flag[index] = False\n",
    "                if append_flag[index]:\n",
    "                    output_control_sequences[index].append(ni)\n",
    "\n",
    "            decoder_input = torch.stack(next_input).unsqueeze(0)\n",
    "\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "\n",
    "            index_control += 1\n",
    "            if index_control >= max_length:\n",
    "                break\n",
    "\n",
    "        return output_control_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '../data/small/'\n",
    "training_set = np.load(base_path + 'beethoven_brunomars_eminem_mozart.npy')\n",
    "train_size = len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS ONLY MANUALLY! \"\"\"\n",
    "meta_learner = MetaLearner(Model, (vocabulary_size, encoding_size), 1, 0.001, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Convert to one-hot \"\"\"\n",
    "SEQ_LENGTH = 250\n",
    "training_one_hot = []\n",
    "training_categories = []\n",
    "for i in range(train_size):\n",
    "    training_categories.append(np.concatenate(([SOS_TOKEN], training_set[i][0:SEQ_LENGTH], [EOS_TOKEN])))\n",
    "    training_one_hot.append(one_hot_embeddings[training_categories[i]])\n",
    "    \n",
    "training_one_hot = np.array(training_one_hot)\n",
    "training_categories = np.array(training_categories, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 252, 4711)\n",
      "(101, 252)\n"
     ]
    }
   ],
   "source": [
    "print(training_one_hot.shape)\n",
    "print(training_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" CAUTION: RUN THIS ONLY MANUALLY! \"\"\"\n",
    "meta_learner.load_state_dict(torch.load(\"../models/maml_combined\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20m 14s (- 465m 32s) (1 4%) 505.4966\n",
      "40m 32s (- 445m 55s) (2 8%) 521.8807\n",
      "60m 50s (- 425m 54s) (3 12%) 486.5450\n",
      "81m 8s (- 405m 44s) (4 16%) 478.9368\n",
      "101m 27s (- 385m 31s) (5 20%) 467.0125\n",
      "121m 45s (- 365m 15s) (6 25%) 537.2839\n",
      "142m 3s (- 344m 59s) (7 29%) 457.8210\n",
      "162m 21s (- 324m 42s) (8 33%) 448.3797\n",
      "182m 40s (- 304m 27s) (9 37%) 481.5830\n",
      "203m 0s (- 284m 12s) (10 41%) 475.0951\n",
      "223m 18s (- 263m 54s) (11 45%) 485.0946\n",
      "243m 35s (- 243m 35s) (12 50%) 477.2808\n",
      "263m 53s (- 223m 17s) (13 54%) 460.3866\n",
      "284m 13s (- 203m 0s) (14 58%) 477.5971\n",
      "304m 33s (- 182m 43s) (15 62%) 465.0226\n",
      "324m 51s (- 162m 25s) (16 66%) 417.0749\n",
      "345m 10s (- 142m 7s) (17 70%) 447.4723\n",
      "365m 28s (- 121m 49s) (18 75%) 485.9039\n",
      "385m 46s (- 101m 31s) (19 79%) 461.2885\n",
      "406m 10s (- 81m 14s) (20 83%) 451.2395\n",
      "426m 28s (- 60m 55s) (21 87%) 452.3545\n",
      "446m 46s (- 40m 36s) (22 91%) 460.0591\n",
      "467m 4s (- 20m 18s) (23 95%) 469.0420\n",
      "487m 21s (- 0m 0s) (24 100%) 459.0662\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 24\n",
    "TASK_BATCH_SIZE = 4 + 1 # 3 support + 1 query\n",
    "SEQ_LENGTH = 252 # SOS and EOS\n",
    "\n",
    "print_every = 1\n",
    "check_every = 1\n",
    "print_loss_total = 0\n",
    "startTime = time.time()\n",
    "randomize_song_index = np.arange(train_size)\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    # Randomize\n",
    "    np.random.shuffle(randomize_song_index)\n",
    "    training_one_hot = training_one_hot[randomize_song_index]\n",
    "    training_categories = training_categories[randomize_song_index]\n",
    "    \n",
    "    for task_batch in range(train_size // TASK_BATCH_SIZE):\n",
    "        batch_start = task_batch * TASK_BATCH_SIZE\n",
    "        batch_end = batch_start + TASK_BATCH_SIZE\n",
    "        batch = training_one_hot[batch_start:batch_end]\n",
    "        \n",
    "        support = batch[:-1]\n",
    "        support_cat = training_categories[batch_start:batch_end-1]\n",
    "        query = batch[-1:]\n",
    "        query_cat = training_categories[batch_end-1:batch_end]\n",
    "        \n",
    "        support = Variable(torch.from_numpy(support)).view(TASK_BATCH_SIZE-1, SEQ_LENGTH, 1, vocabulary_size)\n",
    "        support_cat = Variable(torch.from_numpy(support_cat)).view(TASK_BATCH_SIZE-1, SEQ_LENGTH, 1)\n",
    "        query = Variable(torch.from_numpy(query)).view(1, SEQ_LENGTH, 1, vocabulary_size)\n",
    "        query_cat = Variable(torch.from_numpy(query_cat)).view(1, SEQ_LENGTH, 1)\n",
    "        if use_cuda:\n",
    "            support = support.cuda()\n",
    "            support_cat = support_cat.cuda()\n",
    "            query = query.cuda()\n",
    "            query_cat = query_cat.cuda()\n",
    "        # support_x : (T_i, seq_length, batch_size, vocab_size)\n",
    "        # query_x : (T_i, seq_length, batch_size, vocab_size)\n",
    "        # num_seq : (T_i, seq_length, batch_size)\n",
    "        \n",
    "        loss = meta_learner(support, query, support_cat, query_cat)\n",
    "        print_loss_total += np.sum(np.array(loss))\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(startTime, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg)) \n",
    "        \n",
    "    if epoch % check_every == 0:\n",
    "        # torch.save(meta_learner.state_dict(), '../models/maml_'+str(epoch))\n",
    "        torch.save(meta_learner.state_dict(), '../models/maml_combined')\n",
    "        torch.save(meta_learner.learner.meta_net.state_dict(), '../models/maml_meta_net')\n",
    "        torch.save(meta_learner.learner.learner_net.state_dict(), '../models/maml_learner_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20m 13s (- 465m 2s) (1 4%) 428.4480\n",
      "40m 31s (- 445m 47s) (2 8%) 428.7610\n",
      "60m 43s (- 425m 1s) (3 12%) 459.8260\n",
      "80m 55s (- 404m 39s) (4 16%) 446.7652\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 24\n",
    "TASK_BATCH_SIZE = 4 + 1 # 3 support + 1 query\n",
    "SEQ_LENGTH = 252 # SOS and EOS\n",
    "\n",
    "print_every = 1\n",
    "check_every = 1\n",
    "print_loss_total = 0\n",
    "startTime = time.time()\n",
    "randomize_song_index = np.arange(train_size)\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    # Randomize\n",
    "    np.random.shuffle(randomize_song_index)\n",
    "    training_one_hot = training_one_hot[randomize_song_index]\n",
    "    training_categories = training_categories[randomize_song_index]\n",
    "    \n",
    "    for task_batch in range(train_size // TASK_BATCH_SIZE):\n",
    "        batch_start = task_batch * TASK_BATCH_SIZE\n",
    "        batch_end = batch_start + TASK_BATCH_SIZE\n",
    "        batch = training_one_hot[batch_start:batch_end]\n",
    "        \n",
    "        support = batch[:-1]\n",
    "        support_cat = training_categories[batch_start:batch_end-1]\n",
    "        query = batch[-1:]\n",
    "        query_cat = training_categories[batch_end-1:batch_end]\n",
    "        \n",
    "        support = Variable(torch.from_numpy(support)).view(TASK_BATCH_SIZE-1, SEQ_LENGTH, 1, vocabulary_size)\n",
    "        support_cat = Variable(torch.from_numpy(support_cat)).view(TASK_BATCH_SIZE-1, SEQ_LENGTH, 1)\n",
    "        query = Variable(torch.from_numpy(query)).view(1, SEQ_LENGTH, 1, vocabulary_size)\n",
    "        query_cat = Variable(torch.from_numpy(query_cat)).view(1, SEQ_LENGTH, 1)\n",
    "        if use_cuda:\n",
    "            support = support.cuda()\n",
    "            support_cat = support_cat.cuda()\n",
    "            query = query.cuda()\n",
    "            query_cat = query_cat.cuda()\n",
    "        # support_x : (T_i, seq_length, batch_size, vocab_size)\n",
    "        # query_x : (T_i, seq_length, batch_size, vocab_size)\n",
    "        # num_seq : (T_i, seq_length, batch_size)\n",
    "        \n",
    "        loss = meta_learner(support, query, support_cat, query_cat)\n",
    "        print_loss_total += np.sum(np.array(loss))\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(startTime, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg)) \n",
    "        \n",
    "    if epoch % check_every == 0:\n",
    "        # torch.save(meta_learner.state_dict(), '../models/maml_'+str(epoch))\n",
    "        torch.save(meta_learner.state_dict(), '../models/maml_combined')\n",
    "        torch.save(meta_learner.learner.meta_net.state_dict(), '../models/maml_meta_net')\n",
    "        torch.save(meta_learner.learner.learner_net.state_dict(), '../models/maml_learner_net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" CAUTION: RUN THIS ONLY MANUALLY! \"\"\"\n",
    "meta_learner.load_state_dict(torch.load(\"../models/maml_4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '../data/small/'\n",
    "testing_set = np.load(base_path + 'tchaikovsky.npy')\n",
    "test_size = len(testing_set)\n",
    "\n",
    "\"\"\" Convert to one-hot \"\"\"\n",
    "SEQ_LENGTH = 250\n",
    "testing_one_hot = []\n",
    "testing_categories = []\n",
    "for i in range(test_size):\n",
    "    testing_categories.append(np.concatenate(([SOS_TOKEN], testing_set[i][0:SEQ_LENGTH], [EOS_TOKEN])))\n",
    "    testing_one_hot.append(one_hot_embeddings[testing_categories[i]])\n",
    "    \n",
    "testing_one_hot = np.array(testing_one_hot)\n",
    "testing_categories = np.array(testing_categories, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 252, 4711)\n",
      "(12, 252)\n"
     ]
    }
   ],
   "source": [
    "print(testing_one_hot.shape)\n",
    "print(testing_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "eval_model = Model(vocabulary_size, encoding_size)\n",
    "eval_model.load_state_dict(copy.deepcopy(meta_learner.learner.learner_net.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.528617305126565\n",
      "5.508127835133933\n",
      "5.482793212535819\n",
      "5.458594247550556\n",
      "5.431769151815051\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train on same song 5 times \"\"\"\n",
    "SEQ_LENGTH = 252 # SOS and EOS\n",
    "optimizer = torch.optim.SGD(eval_model.parameters(), 0.001)\n",
    "\n",
    "eval_model.train()\n",
    "for _ in range(5):\n",
    "    sequence = Variable(torch.from_numpy(testing_one_hot[0])).view(SEQ_LENGTH, 1, vocabulary_size)\n",
    "    category = Variable(torch.from_numpy(testing_categories[0])).view(SEQ_LENGTH, 1)\n",
    "    if use_cuda:\n",
    "        sequence = sequence.cuda()\n",
    "        category = category.cuda()\n",
    "        \n",
    "    loss = eval_model(sequence, category)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" MAP Inference on the same song \"\"\"\n",
    "eval_model.eval()\n",
    "TEST_LENGTH = 100\n",
    "SONG_INDEX = 0\n",
    "sequence = Variable(torch.from_numpy(testing_one_hot[SONG_INDEX][0:TEST_LENGTH])).view(TEST_LENGTH, 1, vocabulary_size)\n",
    "if use_cuda:\n",
    "    sequence = sequence.cuda()\n",
    "out_sequence = eval_model.map_inference(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349,)\n"
     ]
    }
   ],
   "source": [
    "from loaders import *\n",
    "_loader = Loader(500)\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "whole_sequence = np.concatenate((testing_categories[SONG_INDEX][0:TEST_LENGTH], np.array(out_sequence[0])))\n",
    "print(whole_sequence.shape)\n",
    "midi_sequence = loader.detokenize(whole_sequence)\n",
    "midi_sequence.write('test.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
