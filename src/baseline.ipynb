{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders import *\n",
    "from dataset import *\n",
    "from episode import *\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = load_sampler_from_config(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Some global variables \"\"\"\n",
    "_loader = Loader(502) # 500 + SOS + EOS\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Is the tokenizer 1 indexed?\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2 # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "\n",
    "encoding_size = 500\n",
    "one_hot_embeddings = np.eye(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "        \n",
    "    def forward(self, input, hidden_in):\n",
    "        _, hidden_out = self.lstm(input, hidden_in) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self, hidden, batch_size):\n",
    "        \n",
    "        if hidden == None:\n",
    "            result = Variable(torch.zeros(1, batch_size, self.hidden_size)).double()\n",
    "            \n",
    "            if use_cuda:\n",
    "                result = result.cuda()\n",
    "            return result\n",
    "        \n",
    "        else:\n",
    "            return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze()\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(1, batch_size, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.double()\n",
    "    loss = losses.sum() / length.double().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size):\n",
    "        super(MetaLearner,self).__init__()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 learning_rate,\n",
    "                 embeddings=one_hot_embeddings):\n",
    "        \n",
    "        super(Learner,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = EncoderLSTM(input_size, hidden_size)\n",
    "        self.decoder = DecoderLSTM(input_size, hidden_size, output_size)\n",
    "        self.encoder_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "        self.decoder_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, sequence, numbered_seq, hidden):\n",
    "        \n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "        embeddings = self.embeddings\n",
    "        criterion = self.criterion\n",
    "        \n",
    "        # (seq_length, batch_size, vocab_size)\n",
    "        seq_size = sequence.size()\n",
    "        batch_size = seq_size[1]\n",
    "        sequence_length = seq_size[0]\n",
    "        loss = 0\n",
    "        \n",
    "        # h_n = (1, batch_size, encoding_size)\n",
    "        encoder_hidden = encoder.initHidden(hidden, batch_size)\n",
    "        encoder_hidden = (encoder_hidden, encoder_hidden) # Need a tuple\n",
    "\n",
    "        # Encoder is fed the flipped control sequence\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            encoder_input = sequence[index_control].unsqueeze(0) # (1, batch_size, vocab_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
    "        \n",
    "        # feed encoder_hidden\n",
    "        decoder_input = sequence[1].unsqueeze(0) # One after SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "        predicted_note_index = 0\n",
    "        \n",
    "        # Prepare the results tensor\n",
    "        all_decoder_outputs = Variable(torch.zeros(*sequence.size())).double() # (seq_length, batch_size, vocab_size)\n",
    "        if use_cuda:\n",
    "            all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "        all_decoder_outputs[0] = decoder_input\n",
    "\n",
    "        for index_control in range(2, sequence_length):\n",
    "            # decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            if random.random() <= 0.9:\n",
    "                decoder_input = sequence[index_control].unsqueeze(0)\n",
    "            else:\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                # This is the next input, without teacher forcing it's the predicted output\n",
    "                decoder_input = torch.stack([Variable(torch.DoubleTensor(embeddings[ni]))\n",
    "                                         for ni in topi.squeeze()]).unsqueeze(0)\n",
    "                if use_cuda:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "                    \n",
    "            # Save the decoder output\n",
    "            all_decoder_outputs[index_control] = decoder_output\n",
    "        \n",
    "        \n",
    "        seq_lens = Variable(torch.LongTensor(np.ones(batch_size, dtype=int)*sequence_length))\n",
    "        if use_cuda:\n",
    "            seq_lens = seq_lens.cuda()\n",
    "        loss = compute_loss(all_decoder_outputs.transpose(0,1).contiguous(),\n",
    "                        numbered_seq.transpose(0,1).contiguous(), \n",
    "                        seq_lens)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def map_inference(self, sequence, hidden, embeddings=one_hot_embeddings, max_length=500):\n",
    "        \"\"\" sequence has to be batch_size=1\"\"\"\n",
    "        encoder = self.encoder\n",
    "        decoder = self.decoder\n",
    "        \n",
    "        output_control_sequence = []\n",
    "    \n",
    "        # Encoder\n",
    "        encoder_hidden = encoder.initHidden(hidden, batch_size=1)\n",
    "        encoder_hidden = (encoder_hidden, encoder_hidden)\n",
    "\n",
    "        sequence_length = sequence.size()[1]\n",
    "\n",
    "        for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "            print(\"Vocab: %d\" % vocabulary_size)\n",
    "            print(\"Idx: %d\" % index_control)\n",
    "            print(sequence)\n",
    "            tmp = sequence[0][index_control]\n",
    "            print(tmp)\n",
    "            encoder_input = tmp.view(1, 1, vocabulary_size)\n",
    "            encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input\n",
    "\n",
    "        # This point we have last encoder_hidden, feed into decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = sequence[0][0]\n",
    "        predicted_control_index = SOS_TOKEN\n",
    "\n",
    "        cur_length = 0\n",
    "        while True:\n",
    "            decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # MAP inference\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            predicted_control_index = int(topi)\n",
    "            if predicted_control_index == EOS_TOKEN:\n",
    "                break\n",
    "            output_control_sequence.append(predicted_control_index)\n",
    "\n",
    "            # This is the next input\n",
    "            decoder_input = torch.from_numpy(embeddings[predicted_control_index])\n",
    "            decoder_input = Variable(decoder_input).double()\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "\n",
    "            cur_length += 1\n",
    "            if cur_length >= max_length:\n",
    "                break\n",
    "\n",
    "        return output_control_sequence\n",
    "    \n",
    "    def train(self, sequence, numbered_seq, hidden):\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.forward(sequence, numbered_seq, hidden)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        return loss\n",
    "\n",
    "learner = Learner(vocabulary_size, \n",
    "              encoding_size, \n",
    "              vocabulary_size,\n",
    "              learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olivia Newton John letsgetphysical.mid\n",
      "[4248, 557, 4251, 560, 4609, 4281, 673, 4613, 2605, 4610, 2608, 2721, 4625, 4249, 560, 557, 4609, 4284, 673, 4632, 4252, 562, 4249, 559, 4609, 2721, 4250, 579, 4249, 576, 4250, 583, 4281, 675, 4609, 2608, 4609, 2605, 4614, 2627, 579, 4610, 2624, 2627, 4249, 576, 4609, 2631, 4250, 583, 4609, 2624, 2631, 4624, 2610, 4610, 2607, 2723, 4626, 4252, 564, 4609, 4251, 560, 4283, 676, 4631, 2608, 4610, 4248, 579, 4249, 576, 583, 4609, 2724, 4609, 2612, 4625, 2627, 2624, 4248, 579, 4249, 576, 4609, 2631, 2627, 2624, 583, 4610, 2631, 4610, 4252, 561, 4249, 565, 4609, 677, 4633, 2725, 562, 678, 4252, 566, 4609, 2609, 4610, 2613, 4677, 2610, 4611, 578, 4247, 581, 4248, 574, 4613, 2726, 4612, 2614, 4610, 2626, 4252, 578, 4609, 2629, 2622, 4247, 581, 4248, 574, 4609, 2626, 4609, 2629, 2622, 4619, 4245, 564, 4281, 680, 4614, 2612, 4627, 2728, 4253, 567, 4284, 683, 4633, 4251, 578, 4248, 581, 574, 4623, 2626, 4251, 578, 4609, 2629, 2626, 4248, 581, 4609, 2622, 574, 4609, 2629, 2622, 4614, 2615, 4609, 2731, 4250, 564, 4283, 680, 4622, 2612, 4643, 2728, 4248, 560, 4609, 4280, 673, 4247, 557, 4614, 2721, 4610, 2608, 2605, 4625, 4246, 560, 4247, 557, 4279, 673, 4629, 2608, 4609, 2605, 4610, 4250, 583, 4248, 576, 4609, 4249, 579, 4250, 559, 4252, 562, 4280, 675, 4612, 2721, 4615, 2627, 4249, 579, 4610, 2627, 4609, 2631, 4250, 583, 4609, 2624, 4248, 576, 4609, 2631, 4609, 2624, 4616, 2607, 4613, 2610, 4627, 4251, 564, 4609, 2723, 560, 4283, 676, 4626, 2608, 4615, 2724, 4246, 579, 4248, 576, 583, 4612, 2612, 4615, 2627, 4246, 579, 4610, 2627, 4610, 2624, 2631, 4248, 576, 583, 4610, 2624, 2631, 4615, 4250, 561, 4249, 565, 4609, 677, 4632, 4254, 566, 4609, 2725, 4284, 678, 4251, 562, 4609, 2609, 4610, 2613, 4675, 2726, 4612, 578, 4248, 574, 4247, 581, 4609, 2610, 4616, 2626, 4251, 578, 4609, 2626, 4611, 2622, 2629, 4248, 574, 4247, 581, 4609, 2622, 2629, 4620, 4246, 564, 680, 4610, 2614, 4612, 2612, 4627, 2728, 4254, 567, 4283, 683, 4633, 4248, 578, 4246, 574, 4248, 581, 4622, 2626, 578, 4609, 2622, 4246, 574, 4609, 2629, 2626, 4248, 581, 4609, 2622, 4609, 2629, 4613, 2615, 4610, 4250, 564, 680, 4610, 2731, 4622, 2612, 4637, 2728, 4612, 4279, 673, 4609, 4244, 557, 4249, 560, 4614, 2605, 4609, 2608, 4611, 2721, 4623, 557, 4250, 560, 673, 4628, 2605, 4609, 2608, 4612, 4247, 579, 576, 4249, 583, 4250, 559, 4280, 675, 562, 4613, 2721, 4619, 2627, 4247, 579, 4610, 2624, 2631, 2627, 576, 4249, 583, 4610, 2624, 2631, 4610, 2607, 4615, 2723, 4618, 2610, 4619, 560, 4251, 564, 4283, 676, 4631, 2608, 4609, 2612, 4609, 4248, 579, 4249, 583, 4247, 576, 4613, 2724, 4618, 2627, 4248, 579, 4609, 2631, 2624, 2627, 4249, 583, 4247, 576, 4610, 2631, 2624, 4614, 4250, 561, 4609, 4246, 565, 677, 4627, 2613, 4614, 4254, 566, 4284, 678, 4251, 562, 4609, 2609, 2725, 4681, 578, 4247, 581, 4246, 574, 4610, 2614, 4609, 2726, 4618, 2626, 4251, 578, 4609, 2629, 2622, 2626]\n",
      "[4248  557 4251  560 4609 4281  673 4613 2605 4610 2608 2721 4625 4249\n",
      "  560  557 4609 4284  673 4632 4252  562 4249  559 4609 2721 4250  579\n",
      " 4249  576 4250  583 4281  675 4609 2608 4609 2605 4614 2627  579 4610\n",
      " 2624 2627 4249  576 4609 2631 4250  583 4609 2624 2631 4624 2610 4610\n",
      " 2607 2723 4626 4252  564 4609 4251  560 4283  676 4631 2608 4610 4248\n",
      "  579 4249  576  583 4609 2724 4609 2612 4625 2627 2624 4248  579 4249\n",
      "  576 4609 2631 2627 2624  583 4610 2631 4610 4252  561 4249  565 4609\n",
      "  677 4633 2725  562  678 4252  566 4609 2609 4610 2613 4677 2610 4611\n",
      "  578 4247  581 4248  574 4613 2726 4612 2614 4610 2626 4252  578 4609\n",
      " 2629 2622 4247  581 4248  574 4609 2626 4609 2629 2622 4619 4245  564\n",
      " 4281  680 4614 2612 4627 2728 4253  567 4284  683 4633 4251  578 4248\n",
      "  581  574 4623 2626 4251  578 4609 2629 2626 4248  581 4609 2622  574\n",
      " 4609 2629 2622 4614 2615 4609 2731 4250  564 4283  680 4622 2612 4643\n",
      " 2728 4248  560 4609 4280  673 4247  557 4614 2721 4610 2608 2605 4625\n",
      " 4246  560 4247  557 4279  673 4629 2608 4609 2605 4610 4250  583 4248\n",
      "  576 4609 4249  579 4250  559 4252  562 4280  675 4612 2721 4615 2627\n",
      " 4249  579 4610 2627 4609 2631 4250  583 4609 2624 4248  576 4609 2631\n",
      " 4609 2624 4616 2607 4613 2610 4627 4251  564 4609 2723  560 4283  676\n",
      " 4626 2608 4615 2724 4246  579 4248  576  583 4612 2612 4615 2627 4246\n",
      "  579 4610 2627 4610 2624 2631 4248  576  583 4610 2624 2631 4615 4250\n",
      "  561 4249  565 4609  677 4632 4254  566 4609 2725 4284  678 4251  562\n",
      " 4609 2609 4610 2613 4675 2726 4612  578 4248  574 4247  581 4609 2610\n",
      " 4616 2626 4251  578 4609 2626 4611 2622 2629 4248  574 4247  581 4609\n",
      " 2622 2629 4620 4246  564  680 4610 2614 4612 2612 4627 2728 4254  567\n",
      " 4283  683 4633 4248  578 4246  574 4248  581 4622 2626  578 4609 2622\n",
      " 4246  574 4609 2629 2626 4248  581 4609 2622 4609 2629 4613 2615 4610\n",
      " 4250  564  680 4610 2731 4622 2612 4637 2728 4612 4279  673 4609 4244\n",
      "  557 4249  560 4614 2605 4609 2608 4611 2721 4623  557 4250  560  673\n",
      " 4628 2605 4609 2608 4612 4247  579  576 4249  583 4250  559 4280  675\n",
      "  562 4613 2721 4619 2627 4247  579 4610 2624 2631 2627  576 4249  583\n",
      " 4610 2624 2631 4610 2607 4615 2723 4618 2610 4619  560 4251  564 4283\n",
      "  676 4631 2608 4609 2612 4609 4248  579 4249  583 4247  576 4613 2724\n",
      " 4618 2627 4248  579 4609 2631 2624 2627 4249  583 4247  576 4610 2631\n",
      " 2624 4614 4250  561 4609 4246  565  677 4627 2613 4614 4254  566 4284\n",
      "  678 4251  562 4609 2609 2725 4681  578 4247  581 4246  574 4610 2614\n",
      " 4609 2726 4618 2626 4251  578 4609 2629 2622 2626]\n",
      "13667\n",
      "9976\n",
      "13670\n",
      "9979\n",
      "14028\n",
      "13700\n",
      "10092\n",
      "14032\n",
      "12024\n",
      "14029\n",
      "12027\n",
      "12140\n",
      "14044\n",
      "13668\n",
      "9979\n",
      "9976\n",
      "14028\n",
      "13703\n",
      "10092\n",
      "14051\n",
      "13671\n",
      "9981\n",
      "13668\n",
      "9978\n",
      "14028\n",
      "12140\n",
      "13669\n",
      "9998\n",
      "13668\n",
      "9995\n",
      "13669\n",
      "10002\n",
      "13700\n",
      "10094\n",
      "14028\n",
      "12027\n",
      "14028\n",
      "12024\n",
      "14033\n",
      "12046\n",
      "9998\n",
      "14029\n",
      "12043\n",
      "12046\n",
      "13668\n",
      "9995\n",
      "14028\n",
      "12050\n",
      "13669\n",
      "10002\n",
      "14028\n",
      "12043\n",
      "12050\n",
      "14043\n",
      "12029\n",
      "14029\n",
      "12026\n",
      "12142\n",
      "14045\n",
      "13671\n",
      "9983\n",
      "14028\n",
      "13670\n",
      "9979\n",
      "13702\n",
      "10095\n",
      "14050\n",
      "12027\n",
      "14029\n",
      "13667\n",
      "9998\n",
      "13668\n",
      "9995\n",
      "10002\n",
      "14028\n",
      "12143\n",
      "14028\n",
      "12031\n",
      "14044\n",
      "12046\n",
      "12043\n",
      "13667\n",
      "9998\n",
      "13668\n",
      "9995\n",
      "14028\n",
      "12050\n",
      "12046\n",
      "12043\n",
      "10002\n",
      "14029\n",
      "12050\n",
      "14029\n",
      "13671\n",
      "9980\n",
      "13668\n",
      "9984\n",
      "14028\n",
      "10096\n",
      "14052\n",
      "12144\n",
      "9981\n",
      "10097\n",
      "13671\n",
      "9985\n",
      "14028\n",
      "12028\n",
      "14029\n",
      "12032\n",
      "14096\n",
      "12029\n",
      "14030\n",
      "9997\n",
      "13666\n",
      "10000\n",
      "13667\n",
      "9993\n",
      "14032\n",
      "12145\n",
      "14031\n",
      "12033\n",
      "14029\n",
      "12045\n",
      "13671\n",
      "9997\n",
      "14028\n",
      "12048\n",
      "12041\n",
      "13666\n",
      "10000\n",
      "13667\n",
      "9993\n",
      "14028\n",
      "12045\n",
      "14028\n",
      "12048\n",
      "12041\n",
      "14038\n",
      "13664\n",
      "9983\n",
      "13700\n",
      "10099\n",
      "14033\n",
      "12031\n",
      "14046\n",
      "12147\n",
      "13672\n",
      "9986\n",
      "13703\n",
      "10102\n",
      "14052\n",
      "13670\n",
      "9997\n",
      "13667\n",
      "10000\n",
      "9993\n",
      "14042\n",
      "12045\n",
      "13670\n",
      "9997\n",
      "14028\n",
      "12048\n",
      "12045\n",
      "13667\n",
      "10000\n",
      "14028\n",
      "12041\n",
      "9993\n",
      "14028\n",
      "12048\n",
      "12041\n",
      "14033\n",
      "12034\n",
      "14028\n",
      "12150\n",
      "13669\n",
      "9983\n",
      "13702\n",
      "10099\n",
      "14041\n",
      "12031\n",
      "14062\n",
      "12147\n",
      "13667\n",
      "9979\n",
      "14028\n",
      "13699\n",
      "10092\n",
      "13666\n",
      "9976\n",
      "14033\n",
      "12140\n",
      "14029\n",
      "12027\n",
      "12024\n",
      "14044\n",
      "13665\n",
      "9979\n",
      "13666\n",
      "9976\n",
      "13698\n",
      "10092\n",
      "14048\n",
      "12027\n",
      "14028\n",
      "12024\n",
      "14029\n",
      "13669\n",
      "10002\n",
      "13667\n",
      "9995\n",
      "14028\n",
      "13668\n",
      "9998\n",
      "13669\n",
      "9978\n",
      "13671\n",
      "9981\n",
      "13699\n",
      "10094\n",
      "14031\n",
      "12140\n",
      "14034\n",
      "12046\n",
      "13668\n",
      "9998\n",
      "14029\n",
      "12046\n",
      "14028\n",
      "12050\n",
      "13669\n",
      "10002\n",
      "14028\n",
      "12043\n",
      "13667\n",
      "9995\n",
      "14028\n",
      "12050\n",
      "14028\n",
      "12043\n",
      "14035\n",
      "12026\n",
      "14032\n",
      "12029\n",
      "14046\n",
      "13670\n",
      "9983\n",
      "14028\n",
      "12142\n",
      "9979\n",
      "13702\n",
      "10095\n",
      "14045\n",
      "12027\n",
      "14034\n",
      "12143\n",
      "13665\n",
      "9998\n",
      "13667\n",
      "9995\n",
      "10002\n",
      "14031\n",
      "12031\n",
      "14034\n",
      "12046\n",
      "13665\n",
      "9998\n",
      "14029\n",
      "12046\n",
      "14029\n",
      "12043\n",
      "12050\n",
      "13667\n",
      "9995\n",
      "10002\n",
      "14029\n",
      "12043\n",
      "12050\n",
      "14034\n",
      "13669\n",
      "9980\n",
      "13668\n",
      "9984\n",
      "14028\n",
      "10096\n",
      "14051\n",
      "13673\n",
      "9985\n",
      "14028\n",
      "12144\n",
      "13703\n",
      "10097\n",
      "13670\n",
      "9981\n",
      "14028\n",
      "12028\n",
      "14029\n",
      "12032\n",
      "14094\n",
      "12145\n",
      "14031\n",
      "9997\n",
      "13667\n",
      "9993\n",
      "13666\n",
      "10000\n",
      "14028\n",
      "12029\n",
      "14035\n",
      "12045\n",
      "13670\n",
      "9997\n",
      "14028\n",
      "12045\n",
      "14030\n",
      "12041\n",
      "12048\n",
      "13667\n",
      "9993\n",
      "13666\n",
      "10000\n",
      "14028\n",
      "12041\n",
      "12048\n",
      "14039\n",
      "13665\n",
      "9983\n",
      "10099\n",
      "14029\n",
      "12033\n",
      "14031\n",
      "12031\n",
      "14046\n",
      "12147\n",
      "13673\n",
      "9986\n",
      "13702\n",
      "10102\n",
      "14052\n",
      "13667\n",
      "9997\n",
      "13665\n",
      "9993\n",
      "13667\n",
      "10000\n",
      "14041\n",
      "12045\n",
      "9997\n",
      "14028\n",
      "12041\n",
      "13665\n",
      "9993\n",
      "14028\n",
      "12048\n",
      "12045\n",
      "13667\n",
      "10000\n",
      "14028\n",
      "12041\n",
      "14028\n",
      "12048\n",
      "14032\n",
      "12034\n",
      "14029\n",
      "13669\n",
      "9983\n",
      "10099\n",
      "14029\n",
      "12150\n",
      "14041\n",
      "12031\n",
      "14056\n",
      "12147\n",
      "14031\n",
      "13698\n",
      "10092\n",
      "14028\n",
      "13663\n",
      "9976\n",
      "13668\n",
      "9979\n",
      "14033\n",
      "12024\n",
      "14028\n",
      "12027\n",
      "14030\n",
      "12140\n",
      "14042\n",
      "9976\n",
      "13669\n",
      "9979\n",
      "10092\n",
      "14047\n",
      "12024\n",
      "14028\n",
      "12027\n",
      "14031\n",
      "13666\n",
      "9998\n",
      "9995\n",
      "13668\n",
      "10002\n",
      "13669\n",
      "9978\n",
      "13699\n",
      "10094\n",
      "9981\n",
      "14032\n",
      "12140\n",
      "14038\n",
      "12046\n",
      "13666\n",
      "9998\n",
      "14029\n",
      "12043\n",
      "12050\n",
      "12046\n",
      "9995\n",
      "13668\n",
      "10002\n",
      "14029\n",
      "12043\n",
      "12050\n",
      "14029\n",
      "12026\n",
      "14034\n",
      "12142\n",
      "14037\n",
      "12029\n",
      "14038\n",
      "9979\n",
      "13670\n",
      "9983\n",
      "13702\n",
      "10095\n",
      "14050\n",
      "12027\n",
      "14028\n",
      "12031\n",
      "14028\n",
      "13667\n",
      "9998\n",
      "13668\n",
      "10002\n",
      "13666\n",
      "9995\n",
      "14032\n",
      "12143\n",
      "14037\n",
      "12046\n",
      "13667\n",
      "9998\n",
      "14028\n",
      "12050\n",
      "12043\n",
      "12046\n",
      "13668\n",
      "10002\n",
      "13666\n",
      "9995\n",
      "14029\n",
      "12050\n",
      "12043\n",
      "14033\n",
      "13669\n",
      "9980\n",
      "14028\n",
      "13665\n",
      "9984\n",
      "10096\n",
      "14046\n",
      "12032\n",
      "14033\n",
      "13673\n",
      "9985\n",
      "13703\n",
      "10097\n",
      "13670\n",
      "9981\n",
      "14028\n",
      "12028\n",
      "12144\n",
      "14100\n",
      "9997\n",
      "13666\n",
      "10000\n",
      "13665\n",
      "9993\n",
      "14029\n",
      "12033\n",
      "14028\n",
      "12145\n",
      "14037\n",
      "12045\n",
      "13670\n",
      "9997\n",
      "14028\n",
      "12048\n",
      "12041\n",
      "12045\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 13667 is out of bounds for axis 0 with size 4711",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-379afb17acc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrunc_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is really time consuming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtrunc_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrunc_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtrunc_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 13667 is out of bounds for axis 0 with size 4711"
     ]
    }
   ],
   "source": [
    "#input_files = ['bach_846.mid', 'mz_311_1.mid', 'rac_op3_2.mid']\n",
    "input_files = ['letsgetphysical.mid']\n",
    "input_variables = []\n",
    "original_sequences = []\n",
    "\n",
    "for index, input_file in enumerate(input_files):\n",
    "    orig_seq = loader.read('../data/' + input_file)\n",
    "    orig_seq = loader.tokenize(orig_seq)\n",
    "    \n",
    "    orig_seq2 = eps.dataset.load('Olivia Newton John', 'letsgetphysical.mid')\n",
    "    \n",
    "    trunc_seq = orig_seq[0:500]\n",
    "    \n",
    "    print(trunc_seq)\n",
    "    print(orig_seq2)\n",
    "\n",
    "    trunc_seq = orig_seq2\n",
    " \n",
    "    trunc_seq = [SOS_TOKEN] + trunc_seq + [EOS_TOKEN]\n",
    "    original_sequences.append(trunc_seq)\n",
    "    seq_length = len(trunc_seq)\n",
    "    \n",
    "    for n in list(trunc_seq):\n",
    "        print(n)\n",
    "    \n",
    "    trunc_seq = torch.from_numpy(np.array(one_hot_embeddings[list(trunc_seq)])) # This is really time consuming\n",
    "    trunc_seq = trunc_seq.view(seq_length, vocabulary_size)\n",
    "    trunc_seq = Variable(trunc_seq)\n",
    "    if use_cuda:\n",
    "        trunc_seq = trunc_seq.cuda()\n",
    "    input_variables.append(trunc_seq)\n",
    "    \n",
    "original_sequences = np.array(original_sequences, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olivia Newton John letsgetphysical.mid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4248,  557, 4251,  560, 4609, 4281,  673, 4613, 2605, 4610, 2608,\n",
       "       2721, 4625, 4249,  560,  557, 4609, 4284,  673, 4632, 4252,  562,\n",
       "       4249,  559, 4609, 2721, 4250,  579, 4249,  576, 4250,  583, 4281,\n",
       "        675, 4609, 2608, 4609, 2605, 4614, 2627,  579, 4610, 2624, 2627,\n",
       "       4249,  576, 4609, 2631, 4250,  583, 4609, 2624, 2631, 4624, 2610,\n",
       "       4610, 2607, 2723, 4626, 4252,  564, 4609, 4251,  560, 4283,  676,\n",
       "       4631, 2608, 4610, 4248,  579, 4249,  576,  583, 4609, 2724, 4609,\n",
       "       2612, 4625, 2627, 2624, 4248,  579, 4249,  576, 4609, 2631, 2627,\n",
       "       2624,  583, 4610, 2631, 4610, 4252,  561, 4249,  565, 4609,  677,\n",
       "       4633, 2725,  562,  678, 4252,  566, 4609, 2609, 4610, 2613, 4677,\n",
       "       2610, 4611,  578, 4247,  581, 4248,  574, 4613, 2726, 4612, 2614,\n",
       "       4610, 2626, 4252,  578, 4609, 2629, 2622, 4247,  581, 4248,  574,\n",
       "       4609, 2626, 4609, 2629, 2622, 4619, 4245,  564, 4281,  680, 4614,\n",
       "       2612, 4627, 2728, 4253,  567, 4284,  683, 4633, 4251,  578, 4248,\n",
       "        581,  574, 4623, 2626, 4251,  578, 4609, 2629, 2626, 4248,  581,\n",
       "       4609, 2622,  574, 4609, 2629, 2622, 4614, 2615, 4609, 2731, 4250,\n",
       "        564, 4283,  680, 4622, 2612, 4643, 2728, 4248,  560, 4609, 4280,\n",
       "        673, 4247,  557, 4614, 2721, 4610, 2608, 2605, 4625, 4246,  560,\n",
       "       4247,  557, 4279,  673, 4629, 2608, 4609, 2605, 4610, 4250,  583,\n",
       "       4248,  576, 4609, 4249,  579, 4250,  559, 4252,  562, 4280,  675,\n",
       "       4612, 2721, 4615, 2627, 4249,  579, 4610, 2627, 4609, 2631, 4250,\n",
       "        583, 4609, 2624, 4248,  576, 4609, 2631, 4609, 2624, 4616, 2607,\n",
       "       4613, 2610, 4627, 4251,  564, 4609, 2723,  560, 4283,  676, 4626,\n",
       "       2608, 4615, 2724, 4246,  579, 4248,  576,  583, 4612, 2612, 4615,\n",
       "       2627, 4246,  579, 4610, 2627, 4610, 2624, 2631, 4248,  576,  583,\n",
       "       4610, 2624, 2631, 4615, 4250,  561, 4249,  565, 4609,  677, 4632,\n",
       "       4254,  566, 4609, 2725, 4284,  678, 4251,  562, 4609, 2609, 4610,\n",
       "       2613, 4675, 2726, 4612,  578, 4248,  574, 4247,  581, 4609, 2610,\n",
       "       4616, 2626, 4251,  578, 4609, 2626, 4611, 2622, 2629, 4248,  574,\n",
       "       4247,  581, 4609, 2622, 2629, 4620, 4246,  564,  680, 4610, 2614,\n",
       "       4612, 2612, 4627, 2728, 4254,  567, 4283,  683, 4633, 4248,  578,\n",
       "       4246,  574, 4248,  581, 4622, 2626,  578, 4609, 2622, 4246,  574,\n",
       "       4609, 2629, 2626, 4248,  581, 4609, 2622, 4609, 2629, 4613, 2615,\n",
       "       4610, 4250,  564,  680, 4610, 2731, 4622, 2612, 4637, 2728, 4612,\n",
       "       4279,  673, 4609, 4244,  557, 4249,  560, 4614, 2605, 4609, 2608,\n",
       "       4611, 2721, 4623,  557, 4250,  560,  673, 4628, 2605, 4609, 2608,\n",
       "       4612, 4247,  579,  576, 4249,  583, 4250,  559, 4280,  675,  562,\n",
       "       4613, 2721, 4619, 2627, 4247,  579, 4610, 2624, 2631, 2627,  576,\n",
       "       4249,  583, 4610, 2624, 2631, 4610, 2607, 4615, 2723, 4618, 2610,\n",
       "       4619,  560, 4251,  564, 4283,  676, 4631, 2608, 4609, 2612, 4609,\n",
       "       4248,  579, 4249,  583, 4247,  576, 4613, 2724, 4618, 2627, 4248,\n",
       "        579, 4609, 2631, 2624, 2627, 4249,  583, 4247,  576, 4610, 2631,\n",
       "       2624, 4614, 4250,  561, 4609, 4246,  565,  677, 4627, 2613, 4614,\n",
       "       4254,  566, 4284,  678, 4251,  562, 4609, 2609, 2725, 4681,  578,\n",
       "       4247,  581, 4246,  574, 4610, 2614, 4609, 2726, 4618, 2626, 4251,\n",
       "        578, 4609, 2629, 2622, 2626], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps.dataset.load('Olivia Newton John', 'letsgetphysical.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olivia Newton John helpmemakeitthroughthenight.mid\n",
      "Olivia Newton John ifyoucouldreadmymind.mid\n",
      "Olivia Newton John thepromise(thedolphinsong).mid\n",
      "Olivia Newton John hopelesslydevotedtoyou.mid\n",
      "Olivia Newton John lookatmeimsandradee.mid\n",
      "Olivia Newton John xanadu.mid\n",
      "Olivia Newton John waterunderthebridge.mid\n",
      "Olivia Newton John magic.mid\n",
      "Olivia Newton John angelofthemorning.mid\n",
      "Olivia Newton John longlivelove.mid\n",
      "Olivia Newton John itssoeasy.mid\n",
      "Olivia Newton John theairthatibreathe.mid\n",
      "Olivia Newton John youretheonethatiwant.mid\n",
      "Olivia Newton John ifweonlyhavelove.mid\n",
      "Olivia Newton John letsgetphysical.mid\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 14127 is out of bounds for axis 0 with size 4711",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8c6085042bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrunc_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrunc_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is really time consuming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtrunc_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrunc_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrunc_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 14127 is out of bounds for axis 0 with size 4711"
     ]
    }
   ],
   "source": [
    "input_variables = []\n",
    "original_sequences = []\n",
    "\n",
    "for index in range(290):\n",
    "    ep = eps.get_episode()\n",
    "    orig_seqs = ep.support\n",
    "    \n",
    "    for orig_seq in orig_seqs:\n",
    "        trunc_seq = [SOS_TOKEN] + orig_seq + [EOS_TOKEN]\n",
    "        original_sequences.append(trunc_seq)\n",
    "        seq_length = len(trunc_seq)\n",
    "\n",
    "        trunc_seq = torch.from_numpy(np.array(one_hot_embeddings[trunc_seq])) # This is really time consuming\n",
    "        trunc_seq = trunc_seq.view(seq_length, vocabulary_size)\n",
    "        trunc_seq = Variable(trunc_seq)\n",
    "        if use_cuda:\n",
    "            trunc_seq = trunc_seq.cuda()\n",
    "        input_variables.append(trunc_seq)\n",
    "\n",
    "original_sequences = np.array(original_sequences, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing Learner \"\"\"\n",
    "print_every = 10\n",
    "total_epochs = 200\n",
    "print_loss_total = 0\n",
    "batch_size = 2\n",
    "startTime = time.time()\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    for batch in range(len(input_variables)//batch_size):\n",
    "        # lstm input is (seq_len, batch_size, vocab_size)\n",
    "        start, end = batch*batch_size, (batch+1)*batch_size\n",
    "        sequences = torch.stack(input_variables[start:end]).transpose(0,1)\n",
    "        numbered_seqs = torch.stack(Variable(torch.from_numpy(original_sequences[start:end]))).transpose(0,1)\n",
    "        if use_cuda:\n",
    "            numbered_seqs = numbered_seqs.cuda()\n",
    "        loss = learner.train(sequences, numbered_seqs, hidden=None)\n",
    "        print_loss_total += loss\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(startTime, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_index = 0\n",
    "test_seq = input_variables[song_index][:,1:101] # First dimension is batch\n",
    "out_seq = learner.map_inference(test_seq, hidden=None)\n",
    "whole_seq = original_sequences[song_index][0:100].tolist() + out_seq\n",
    "midi = loader.detokenize(np.array(whole_seq))\n",
    "midi.write('test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hidden = learner.encoder.initHidden(None).squeeze()\n",
    "print(torch.stack([test_hidden, test_hidden]).unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
