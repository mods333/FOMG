{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_loader = Loader(500)\n",
    "loader = MIDILoader(_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_sequence = loader.read('../data/bach_846.mid')\n",
    "sequence = loader.tokenize(_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Is the tokenizer 1 indexed?\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2 # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "encoding_size = 1000\n",
    "one_hot_embeddings = np.eye(vocabulary_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midi_sequence = loader.detokenize(np.array(sequence))\n",
    "midi_sequence.write('test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "        \n",
    "    def forward(self, input, hidden_in):\n",
    "        _, hidden_out = self.lstm(input, hidden_in) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        self.project = nn.Linear(4096, self.hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "            self.project = self.project.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze()\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self, init_size, image_features):\n",
    "        result = self.project(image_features)\n",
    "        result = F.relu(result)\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sequence,\n",
    "          encoder, \n",
    "          decoder, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer, \n",
    "          criterion,\n",
    "          embeddings=one_hot_embeddings):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # target_variable has (batch_size, length, vocab_size)\n",
    "    # Without minibatch, this is just one sequence\n",
    "    sequence_length = sequence.size()[1]\n",
    "    loss = 0\n",
    "    \n",
    "    # Encoder is fed from the flipped sentence\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_hidden = (encoder_hidden, encoder_hidden) # Need a tuple\n",
    "    \n",
    "    # Feeding encoder in a loop, in reverse order\n",
    "    # Skip index=0 which should be SOS\n",
    "    for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "        encoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input    \n",
    "    \n",
    "    # feed encoder_hidden\n",
    "    decoder_input = sequence[0][0]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    predicted_note_index = 0\n",
    "    \n",
    "    for index_control in range(1, sequence_length):\n",
    "        decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_control_index = int(topi)\n",
    "\n",
    "        \n",
    "        if random.random() <= 0.9:\n",
    "            decoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        else:\n",
    "            # This is the next input, without teacher forcing it's the predicted output\n",
    "            decoder_input = torch.from_numpy(embeddings[predicted_control_index])\n",
    "            decoder_input = Variable(decoder_input)\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "        \n",
    "        # This is just to conform with the pytorch format..\n",
    "        # CrossEntropyLoss takes input1: (N, C) and input2: (N).\n",
    "        _, actual_control_index = sequence[0][index_control].topk(1)\n",
    "        if use_cuda:\n",
    "            actual_control_index = actual_control_index.cuda()\n",
    "\n",
    "        # Compare current output to next \"target\" input\n",
    "        loss += criterion(decoder_output, actual_control_index)\n",
    "            \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # index_word keeps track of the current word\n",
    "    # in case of break (EOS) and non-break (teacher-forcing), it'll be the actually count.\n",
    "    return loss.data[0] / index_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_loader = Loader(500)\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "# Initialize the encoder with a hidden size of 1000. \n",
    "# With one-hot, the input size is\n",
    "encoder = EncoderLSTM(vocabulary_size, encoding_size)\n",
    "decoder = DecoderLSTM(vocabulary_size, encoding_size, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8888c8cdf145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_variables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-ee4d8182717b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sequence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, embeddings)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_control\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# This is the next input, without teacher forcing it's the predicted output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input_files = ['bach_846.mid', 'mz_311_1.mid', 'rac_op3_2.mid']\n",
    "input_files = ['bach_846.mid']\n",
    "encoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "input_variables = []\n",
    "\n",
    "for index, input_file in enumerate(input_files):\n",
    "    sequence = loader.read('../data/' + input_file)\n",
    "    sequence = loader.tokenize(sequence)\n",
    "    sequence = [SOS_TOKEN] + sequence + [EOS_TOKEN]\n",
    "    seq_length = len(sequence)\n",
    "    sequence = torch.from_numpy(np.array(one_hot_embeddings[sequence])) # This is really time consuming\n",
    "    sequence = sequence.view(1, seq_length, vocabulary_size)\n",
    "    sequence = Variable(sequence)\n",
    "    if use_cuda:\n",
    "        sequence = sequence.cuda()\n",
    "    input_variables.append(sequence)\n",
    "        \n",
    "for epoch in range(10):\n",
    "    for index, input_file in enumerate(input_files):\n",
    "        sequence = input_variables[index]\n",
    "        train(sequence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_inference(sequence, encoder, decoder, embeddings=one_hot_embeddings, max_length=1000):\n",
    "     \n",
    "    output_control_sequence = []\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_hidden = (encoder_hidden, encoder_hidden)\n",
    "    \n",
    "    sequence_length = sequence.size()[1]\n",
    "    \n",
    "    for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "        encoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input\n",
    "        \n",
    "    # This point we have last encoder_hidden, feed into decoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = sequence[0][0]\n",
    "    predicted_control_index = SOS_TOKEN\n",
    "    \n",
    "    cur_length = 0\n",
    "    while True:\n",
    "        decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        # MAP inference\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_control_index = int(topi)\n",
    "        if predicted_control_index == EOS_TOKEN:\n",
    "            break\n",
    "        output_control_sequence.append(predicted_control_index)\n",
    "        \n",
    "        # This is the next input\n",
    "        decoder_input = torch.from_numpy(embeddings[predicted_control_index])\n",
    "        decoder_input = Variable(decoder_input).double()\n",
    "        if use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "        \n",
    "        cur_length += 1\n",
    "        if cur_length >= max_length:\n",
    "            break\n",
    "        \n",
    "    return output_control_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = loader.read('../data/bach_846.mid')\n",
    "sequence = loader.tokenize(sequence)\n",
    "sequence = sequence[0:500]\n",
    "seq_length = len(sequence)\n",
    "sequence_var = torch.from_numpy(np.array(one_hot_embeddings[sequence])) # This is really time consuming\n",
    "sequence_var = sequence_var.view(1, seq_length, vocabulary_size)\n",
    "sequence_var = Variable(sequence_var)\n",
    "if use_cuda:\n",
    "    sequence_var = sequence_var.cuda()\n",
    "\n",
    "generated_sequence = map_inference(sequence_var, encoder, decoder)\n",
    "whole_sequence = sequence + generated_sequence \n",
    "midi_sequence = loader.detokenize(np.array(whole_sequence))\n",
    "midi_sequence.write('test.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
