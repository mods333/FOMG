{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_loader = Loader(500)\n",
    "loader = MIDILoader(_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_sequence = loader.read('../data/bach_846.mid')\n",
    "sequence = loader.tokenize(_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Is the tokenizer 1 indexed?\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 # 4708 + 1\n",
    "vocabulary_size = vocabulary_size + 2 # SOS (index 4709) and EOS (index 4710)\n",
    "SOS_TOKEN = 4709\n",
    "EOS_TOKEN = 4710\n",
    "encoding_size = 500\n",
    "one_hot_embeddings = np.eye(vocabulary_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midi_sequence = loader.detokenize(np.array(sequence))\n",
    "midi_sequence.write('test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "        \n",
    "    def forward(self, input, hidden_in):\n",
    "        _, hidden_out = self.lstm(input, hidden_in) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        self.project = nn.Linear(4096, self.hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "            self.project = self.project.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze()\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sequence,\n",
    "          encoder, \n",
    "          decoder, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer, \n",
    "          criterion,\n",
    "          embeddings=one_hot_embeddings):\n",
    "    \n",
    "    # encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # target_variable has (batch_size, length, vocab_size)\n",
    "    # Without minibatch, this is just one sequence\n",
    "    sequence_length = sequence.size()[1]\n",
    "    loss = 0\n",
    "    \n",
    "    # Encoder is fed from the flipped sentence\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_hidden = (encoder_hidden, encoder_hidden) # Need a tuple\n",
    "    \n",
    "    # Feeding encoder in a loop, in reverse order\n",
    "    # Skip index=0 which should be SOS\n",
    "    for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "        encoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input    \n",
    "    \n",
    "    # feed encoder_hidden\n",
    "    decoder_input = sequence[0][1] # One after SOS\n",
    "    decoder_hidden = encoder_hidden\n",
    "    predicted_note_index = 0\n",
    "    \n",
    "    for index_control in range(2, sequence_length):\n",
    "        decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_control_index = int(topi)\n",
    "\n",
    "        \n",
    "        if random.random() <= 0.9:\n",
    "            decoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        else:\n",
    "            # This is the next input, without teacher forcing it's the predicted output\n",
    "            decoder_input = torch.from_numpy(embeddings[predicted_control_index])\n",
    "            decoder_input = Variable(decoder_input)\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "        \n",
    "        # This is just to conform with the pytorch format..\n",
    "        # CrossEntropyLoss takes input1: (N, C) and input2: (N).\n",
    "        _, actual_control_index = sequence[0][index_control].topk(1)\n",
    "        if use_cuda:\n",
    "            actual_control_index = actual_control_index.cuda()\n",
    "\n",
    "        # Compare current output to next \"target\" input\n",
    "        loss += criterion(decoder_output, actual_control_index)\n",
    "            \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # index_word keeps track of the current word\n",
    "    # in case of break (EOS) and non-break (teacher-forcing), it'll be the actually count.\n",
    "    return loss.data[0] / index_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_loader = Loader(500)\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "# Initialize the encoder with a hidden size of 1000. \n",
    "# With one-hot, the input size is\n",
    "encoder = EncoderLSTM(vocabulary_size, encoding_size)\n",
    "decoder = DecoderLSTM(vocabulary_size, encoding_size, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_files = ['bach_846.mid', 'mz_311_1.mid', 'rac_op3_2.mid']\n",
    "input_files = ['bach_846.mid']\n",
    "input_variables = []\n",
    "\n",
    "for index, input_file in enumerate(input_files):\n",
    "    sequence = loader.read('../data/' + input_file)\n",
    "    sequence = loader.tokenize(sequence)\n",
    "    sequence = sequence[0:500]\n",
    "    sequence = [SOS_TOKEN] + sequence + [EOS_TOKEN]\n",
    "    seq_length = len(sequence)\n",
    "    sequence = torch.from_numpy(np.array(one_hot_embeddings[sequence])) # This is really time consuming\n",
    "    sequence = sequence.view(1, seq_length, vocabulary_size)\n",
    "    sequence = Variable(sequence)\n",
    "    if use_cuda:\n",
    "        sequence = sequence.cuda()\n",
    "    input_variables.append(sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 38s (- 80m 48s) (10 2%) 4.7163\n",
      "3m 16s (- 78m 30s) (20 4%) 3.4671\n",
      "4m 54s (- 76m 50s) (30 6%) 3.4478\n",
      "6m 31s (- 75m 7s) (40 8%) 3.4384\n",
      "8m 9s (- 73m 21s) (50 10%) 3.4312\n",
      "9m 46s (- 71m 41s) (60 12%) 3.4224\n",
      "11m 23s (- 69m 59s) (70 14%) 3.4082\n",
      "13m 0s (- 68m 18s) (80 16%) 3.3833\n",
      "14m 37s (- 66m 38s) (90 18%) 3.3405\n",
      "16m 14s (- 64m 59s) (100 20%) 3.2662\n",
      "17m 52s (- 63m 23s) (110 22%) 3.1426\n",
      "19m 31s (- 61m 50s) (120 24%) 2.9527\n",
      "21m 10s (- 60m 16s) (130 26%) 2.7730\n",
      "22m 47s (- 58m 37s) (140 28%) 2.5883\n",
      "24m 24s (- 56m 57s) (150 30%) 2.4195\n",
      "26m 3s (- 55m 21s) (160 32%) 2.3164\n",
      "27m 40s (- 53m 42s) (170 34%) 2.1499\n",
      "29m 17s (- 52m 4s) (180 36%) 1.9789\n",
      "30m 54s (- 50m 26s) (190 38%) 1.8290\n",
      "32m 32s (- 48m 48s) (200 40%) 1.6984\n",
      "34m 10s (- 47m 11s) (210 42%) 1.6030\n",
      "35m 47s (- 45m 33s) (220 44%) 1.4616\n",
      "37m 25s (- 43m 55s) (230 46%) 1.3352\n",
      "39m 3s (- 42m 18s) (240 48%) 1.2197\n",
      "40m 41s (- 40m 41s) (250 50%) 1.1250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-47426bf3ce02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-b70e1e0c4a0b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sequence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, embeddings)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_control_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print_every = 10\n",
    "total_epochs = 500\n",
    "print_loss_total = 0\n",
    "start = time.time()\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    for index, sequence in enumerate(input_variables):\n",
    "        loss = train(sequence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / total_epochs),\n",
    "                                     epoch, epoch / total_epochs * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_inference(sequence, encoder, decoder, embeddings=one_hot_embeddings, max_length=1000):\n",
    "     \n",
    "    output_control_sequence = []\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_hidden = (encoder_hidden, encoder_hidden)\n",
    "    \n",
    "    sequence_length = sequence.size()[1]\n",
    "    \n",
    "    for index_control in np.arange(sequence_length-1, 0, -1):\n",
    "        encoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input\n",
    "    \n",
    "    # This point we have last encoder_hidden, feed into decoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = sequence[0][0]\n",
    "    predicted_control_index = SOS_TOKEN\n",
    "    \n",
    "    cur_length = 0\n",
    "    while True:\n",
    "        decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        # MAP inference\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_control_index = int(topi)\n",
    "        if predicted_control_index == EOS_TOKEN:\n",
    "            break\n",
    "        output_control_sequence.append(predicted_control_index)\n",
    "        \n",
    "        # This is the next input\n",
    "        decoder_input = torch.from_numpy(embeddings[predicted_control_index])\n",
    "        decoder_input = Variable(decoder_input).double()\n",
    "        if use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "        \n",
    "        cur_length += 1\n",
    "        if cur_length >= max_length:\n",
    "            break\n",
    "        \n",
    "    return output_control_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence = loader.read('../data/bach_846.mid')\n",
    "sequence = loader.tokenize(sequence)\n",
    "sequence = sequence[0:100]\n",
    "seq_length = len(sequence)\n",
    "sequence_var = torch.from_numpy(np.array(one_hot_embeddings[sequence])) # This is really time consuming\n",
    "sequence_var = sequence_var.view(1, seq_length, vocabulary_size)\n",
    "sequence_var = Variable(sequence_var)\n",
    "if use_cuda:\n",
    "    sequence_var = sequence_var.cuda()\n",
    "\n",
    "generated_sequence = map_inference(sequence_var, encoder, decoder)\n",
    "whole_sequence = sequence + generated_sequence \n",
    "midi_sequence = loader.detokenize(np.array(whole_sequence))\n",
    "midi_sequence.write('test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence = loader.read('../data/bach_846.mid')\n",
    "sequence = loader.tokenize(sequence)\n",
    "sequence = sequence[0:500]\n",
    "orig_sequence = loader.detokenize(np.array(sequence))\n",
    "orig_sequence.write('test2.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4143,\n",
       " 4143,\n",
       " 4143,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628,\n",
       " 4628]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder_model')\n",
    "torch.save(decoder.state_dict(), 'decoder_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
