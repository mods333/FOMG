{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_loader = Loader(500)\n",
    "loader = MIDILoader(_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_sequence = loader.read('../data/bach_846.mid')\n",
    "sequence = loader.tokenize(_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# Is the tokenizer 1 indexed?\n",
    "vocabulary_size = 16*128*2 + 32*16 + 100 + 1 # 4708 + 1\n",
    "encoding_size = 1000\n",
    "one_hot_embeddings = np.eye(vocabulary_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midi_sequence = loader.detokenize(np.array(sequence))\n",
    "midi_sequence.write('test.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "        \n",
    "    def forward(self, input, hidden_in):\n",
    "        _, hidden_out = self.lstm(input, hidden_in) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size)).double()\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double()\n",
    "        self.out = nn.Linear(hidden_size, output_size).double()\n",
    "        self.project = nn.Linear(4096, self.hidden_size).double()\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "            self.project = self.project.cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze()\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self, init_size, image_features):\n",
    "        result = self.project(image_features)\n",
    "        result = F.relu(result)\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sequence,\n",
    "          encoder, \n",
    "          decoder, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer, \n",
    "          criterion,\n",
    "          embeddings=one_hot_embeddings):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # target_variable has (batch_size, length, vocab_size)\n",
    "    # Without minibatch, this is just one sequence\n",
    "    sequence_length = sequence.size()[1]\n",
    "    loss = 0\n",
    "    \n",
    "    # Encoder is fed from the flipped sentence\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_hidden = (encoder_hidden, encoder_hidden) # Need a tuple\n",
    "    \n",
    "    # Feeding encoder in a loop, in reverse order\n",
    "    for index_control in np.arange(sequence_length-1, -1, -1):\n",
    "        encoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input    \n",
    "    \n",
    "    # feed encoder_hidden\n",
    "    decoder_input = sequence[0][0]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    predicted_note_index = 0\n",
    "    \n",
    "    for index_control in range(1, sequence_length):\n",
    "        decoder_input = decoder_input.view(1, 1, vocabulary_size)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_control_index = int(topi)\n",
    "\n",
    "        \n",
    "        if random.random() <= 0.9:\n",
    "            decoder_input = sequence[0][index_control].view(1, 1, vocabulary_size)\n",
    "        else:\n",
    "            # This is the next input, without teacher forcing it's the predicted output\n",
    "            decoder_input = torch.from_numpy(embeddings[predicted_control_index])\n",
    "            decoder_input = Variable(decoder_input)\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "        \n",
    "        # This is just to conform with the pytorch format..\n",
    "        # CrossEntropyLoss takes input1: (N, C) and input2: (N).\n",
    "        _, actual_control_index = sequence[0][index_control].topk(1)\n",
    "        if use_cuda:\n",
    "            actual_control_index = actual_control_index.cuda()\n",
    "\n",
    "        # Compare current output to next \"target\" input\n",
    "        loss += criterion(decoder_output, actual_control_index)\n",
    "            \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # index_word keeps track of the current word\n",
    "    # in case of break (EOS) and non-break (teacher-forcing), it'll be the actually count.\n",
    "    return loss.data[0] / index_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_loader = Loader(500)\n",
    "loader = MIDILoader(_loader)\n",
    "\n",
    "# Initialize the encoder with a hidden size of 1000. \n",
    "# With one-hot, the input size is\n",
    "encoder = EncoderLSTM(vocabulary_size, encoding_size)\n",
    "decoder = DecoderLSTM(vocabulary_size, encoding_size, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['bach_846.mid', 'mz_311_1.mid', 'rac_op3_2.mid']\n",
    "encoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for index, input_file in enumerate(input_files):\n",
    "        sequence = loader.read('../data/' + input_file)\n",
    "        sequence = loader.tokenize(sequence)\n",
    "        seq_length = len(sequence)\n",
    "        sequence = torch.from_numpy(np.array(one_hot_embeddings[sequence])) # This is really time consuming\n",
    "        sequence = sequence.view(1, seq_length, vocabulary_size)\n",
    "        sequence = Variable(sequence)\n",
    "        if use_cuda:\n",
    "            sequence = sequence.cuda()\n",
    "        train(sequence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(sequence, encoder, decoder, embeddings=one_hot_embeddings, max_length=maxSequenceLength):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
